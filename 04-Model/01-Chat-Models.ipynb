{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee483b3",
   "metadata": {},
   "source": [
    "# 다양한 LLM 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf28f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T06:31:11.923493900Z",
     "start_time": "2024-10-01T06:31:11.909346100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd6b300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T06:31:12.383335Z",
     "start_time": "2024-10-01T06:31:11.927494300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH04-Models\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550920c-09d8-48b3-be2f-b36362c37989",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "### 개요\n",
    "OpenAI는 채팅 전용 Large Language Model (LLM)을 제공합니다. 이 모델을 생성할 때 다양한 옵션을 지정할 수 있으며, 이러한 옵션들은 모델의 동작 방식에 영향을 미칩니다.\n",
    "\n",
    "### 옵션 상세 설명\n",
    "\n",
    "`temperature`\n",
    "\n",
    "- 샘플링 온도를 설정하는 옵션입니다. 값은 0과 2 사이에서 선택할 수 있습니다. 높은 값(예: 0.8)은 출력을 더 무작위하게 만들고, 낮은 값(예: 0.2)은 출력을 더 집중되고 결정론적으로 만듭니다.\n",
    "\n",
    "`max_tokens`\n",
    "\n",
    "- 채팅 완성에서 생성할 토큰의 최대 개수를 지정합니다. 이 옵션은 모델이 한 번에 생성할 수 있는 텍스트의 길이를 제어합니다.\n",
    "\n",
    "`model_name`\n",
    "\n",
    "- 적용 가능한 모델을 선택하는 옵션입니다. 더 자세한 정보는 [OpenAI 모델 문서](https://platform.openai.com/docs/models)에서 확인할 수 있습니다.\n",
    "\n",
    "\n",
    "**모델 스펙**\n",
    "\n",
    "- 링크: https://platform.openai.com/docs/models/gpt-4o\n",
    "\n",
    "| Model Name           | Description                                                                                                     | Context Length | Training Data      |\n",
    "|----------------------|-----------------------------------------------------------------------------------------------------------------|----------------|--------------------|\n",
    "| gpt-4o               | 새로운 GPT-4o: GPT-4 터보보다 저렴하고 빠른 최신 다중모드 플래그십 모델입니다.                                      | 128,000 tokens | Up to Oct 2023     |\n",
    "| gpt-4-turbo          | 최신 GPT-4 터보 모델로, 비전 기능이 포함되어 있습니다. JSON 모드와 기능 호출을 사용할 수 있습니다.                | 128,000 tokens | Up to Dec 2023     |\n",
    "| gpt-4                | 최신 GPT-4 모델입니다.                                             | 8,192 tokens   | Up to Sep 2021     |\n",
    "| gpt-3.5-turbo-0125   | 최신 GPT-3.5 터보 모델로, 요청된 형식에 따라 더 정확하게 응답하며 비영어 언어 기능 호출에 관한 텍스트 인코딩 문제를 해결합니다. | 16,385 tokens  | Up to Sep 2021     |\n",
    "| gpt-3.5-turbo        | 현재 gpt-3.5-turbo-0125를 가리킵니다.                                                                           | 16,385 tokens  | Up to Sep 2021     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc161c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T06:31:16.697394Z",
     "start_time": "2024-10-01T06:31:13.148752500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑은 매우 복잡하고 다면적인 감정으로, 사람마다 다르게 정의될 수 있습니다. 일반적으로 사랑은 깊은 애정, 헌신, 그리고 상호 존중을 포함하는 감정입니다. 사랑은 여러 형태로 나타날 수 있습니다:\n",
      "\n",
      "1. **로맨틱한 사랑**: 연인 간의 깊은 애정과 열정을 포함하는 감정입니다.\n",
      "2. **가족 사랑**: 부모와 자식, 형제자매 간의 무조건적인 사랑과 보호 본능을 포함합니다.\n",
      "3. **우정**: 친구 간의 신뢰와 상호 존중을 바탕으로 한 사랑입니다.\n",
      "4. **자기 사랑**: 자신을 존중하고 돌보는 감정입니다.\n",
      "5. **이타적인 사랑**: 다른 사람을 돕고자 하는 무조건적인 사랑입니다.\n",
      "\n",
      "사랑은 기쁨과 행복을 가져다줄 수 있지만, 때로는 고통과 슬픔을 동반하기도 합니다. 사랑은 인간 관계의 중요한 요소로, 우리의 삶에 깊은 영향을 미칩니다."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 객체를 생성합니다.\n",
    "gpt = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = gpt.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2aa412",
   "metadata": {},
   "source": [
    "# Anthropic\n",
    "\n",
    "Anthropic은 인공지능(AI) 안전성과 연구에 중점을 둔 미국의 스타트업 기업입니다. 주요 정보는 다음과 같습니다:\n",
    "\n",
    "- **설립 연도**: 2021년\n",
    "- **위치**: 미국 샌프란시스코\n",
    "- **창립자**: OpenAI 출신 직원들 (Daniela Amodei와 Dario Amodei 등)\n",
    "- **기업 형태**: 공익기업(Public Benefit Corporation)으로 등록\n",
    "\n",
    "## Claude\n",
    "\n",
    "Claude는 Anthropic의 대표적인 대규모 언어 모델(LLM) 제품군입니다. \n",
    "\n",
    "- **API 키 발급**: [https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)\n",
    "- **모델 리스트**: [https://docs.anthropic.com/en/docs/about-claude/models](https://docs.anthropic.com/en/docs/about-claude/models)\n",
    "\n",
    "\n",
    "![](images/anthropic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c7a79",
   "metadata": {},
   "source": [
    "| model_name     | model                                   | Anthropic API                           | AWS Bedrock                            | GCP Vertex AI                 |\n",
    "|----------------|-----------------------------------------|-----------------------------------------|----------------------------------------|-------------------------------|\n",
    "| Claude 3.5 Opus   | 연말 출시 예정                           | 연말 출시 예정                           | 연말 출시 예정                           | 연말 출시 예정                 |\n",
    "| Claude 3.5 Sonnet | claude-3-5-sonnet-20240620              | anthropic.claude-3-5-sonnet-20240620-v1:0 | c1aude-3-5-sonnet@20240620              | claude-3-5-sonnet@20240620     |\n",
    "| Claude 3.5 Haiku  | 연말 출시 예정                           | 연말 출시 예정                           | 연말 출시 예정                           | 연말 출시 예정                 |\n",
    "| Claude 3 Opus     | claude-3-opus-20240229                  | anthropic.claude-3-opus-20240229-v1:0    | c1aude-3-opus@20240229                  | claude-3-opus@20240229         |\n",
    "| Claude 3 Sonnet   | claude-3-sonnet-20240229                | anthropic.claude-3-sonnet-20240229-v1:0  | c1aude-3-sonnet@20240229                | claude-3-sonnet@20240229       |\n",
    "| Claude 3 Haiku    | claude-3-haiku-20240307                 | anthropic.claude-3-haiku-20240307-v1:0   | c1aude-3-haiku@20240307                 | claude-3-haiku@20240307        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b38108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# ChatAnthropic 객체를 생성합니다.\n",
    "anthropic = ChatAnthropic(model_name=\"claude-3-5-sonnet-20240620\")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = anthropic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea6a72",
   "metadata": {},
   "source": [
    "# Cohere\n",
    "\n",
    "Cohere는 기업용 인공지능 솔루션을 제공하는 선도적인 AI 기업으로, 대규모 언어 모델(LLM)을 개발하여 기업들이 AI 기술을 쉽게 도입하고 활용할 수 있도록 돕고 있습니다.\n",
    "\n",
    "## Cohere 개요\n",
    "\n",
    "- **설립연도**: 2020년\n",
    "- **주요 투자자**: Inovia Capital, NVIDIA, Oracle, Salesforce Ventures\n",
    "- **시리즈 C 펀딩**: 2억 7000만 달러 유치\n",
    "- **기업 미션**: 기업용 AI 플랫폼 제공\n",
    "\n",
    "## 주요 제품\n",
    "\n",
    "### Command R+\n",
    "\n",
    "Command R+는 기업용으로 최적화된 Cohere의 최신 LLM입니다. \n",
    "\n",
    "#### 주요 특징\n",
    "\n",
    "- **긴 컨텍스트 윈도우**: 128k 토큰 지원\n",
    "- **고급 RAG 기능**: 검색 강화 생성 기능 제공\n",
    "- **다국어 지원**: 10개 주요 비즈니스 언어 지원\n",
    "- **자동화 도구 사용 기능**: 복잡한 비즈니스 프로세스 자동화\n",
    "\n",
    "### Aya\n",
    "\n",
    "Aya는 Cohere의 비영리 연구소인 Cohere for AI에서 개발한 오픈소스 다국어 LLM입니다. \n",
    "\n",
    "#### 주요 특징\n",
    "\n",
    "- **언어 지원**: 101개 언어 지원 (기존 오픈소스 모델의 두 배 이상)\n",
    "- **훈련 데이터셋**: 5억 1300만 개의 데이터 포인트 포함하는 대규모 다국어 훈련 데이터셋 공개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# ChatCohere 객체를 생성합니다.\n",
    "cohere = ChatCohere(temperature=0)\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = cohere.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640b099",
   "metadata": {},
   "source": [
    "# Upstage\n",
    "\n",
    "Upstage는 인공지능(AI) 기술, 특히 대규모 언어 모델(LLM)과 문서 AI 분야에 특화된 국내 스타트업입니다.\n",
    "\n",
    "## 주요 제품 및 기술\n",
    "\n",
    "### Solar LLM\n",
    "- **주요 특징**: Upstage의 주력 대규모 언어 모델로, 빠른 성능과 비용 효율성으로 주목받고 있습니다.\n",
    "- **기술적 접근**: Depth-Up Scaling (DUS) 기술을 적용하여 성능을 극대화합니다.\n",
    "- **플랫폼 통합**: Amazon SageMaker JumpStart 등 다양한 플랫폼을 통해 API로 통합 제공됩니다.\n",
    "\n",
    "### Document AI Pack\n",
    "- **기능**: OCR 기술을 기반으로 한 문서 처리 솔루션으로, 복잡한 문서에서 필요한 내용을 정확히 추출하고 디지털화합니다.\n",
    "\n",
    "### AskUp Seargest\n",
    "- **특징**: 개인화된 검색 및 추천 서비스를 제공하며, 기존의 ChatGPT 통합 무료 챗봇 AskUp의 업그레이드 버전입니다.\n",
    "\n",
    "## API 키 발급\n",
    "API 키 발급은 [여기](https://console.upstage.ai/api-keys)에서 가능합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc739c29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:16:32.105597900Z",
     "start_time": "2024-09-28T17:16:31.475625500Z"
    }
   },
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'message': 'Unauthorized'}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m answer \u001B[38;5;241m=\u001B[39m upstage\u001B[38;5;241m.\u001B[39mstream(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m사랑이 뭔가요?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# 답변 출력\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[43mstream_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43manswer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_teddynote\\messages.py:20\u001B[0m, in \u001B[0;36mstream_response\u001B[1;34m(response, return_output)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03mAI 모델로부터의 응답을 스트리밍하여 각 청크를 처리하면서 출력합니다.\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03m- str: `return_output`이 True인 경우, 연결된 응답 문자열입니다. 그렇지 않으면, 아무것도 반환되지 않습니다.\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     19\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 20\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mAIMessageChunk\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001B[0m, in \u001B[0;36mBaseChatModel.stream\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    405\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(\n\u001B[0;32m    406\u001B[0m         e,\n\u001B[0;32m    407\u001B[0m         response\u001B[38;5;241m=\u001B[39mLLMResult(\n\u001B[0;32m    408\u001B[0m             generations\u001B[38;5;241m=\u001B[39m[[generation]] \u001B[38;5;28;01mif\u001B[39;00m generation \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[0;32m    409\u001B[0m         ),\n\u001B[0;32m    410\u001B[0m     )\n\u001B[1;32m--> 411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    413\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_llm_end(LLMResult(generations\u001B[38;5;241m=\u001B[39m[[generation]]))\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:391\u001B[0m, in \u001B[0;36mBaseChatModel.stream\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrate_limiter\u001B[38;5;241m.\u001B[39macquire(blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 391\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[0;32m    393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun-\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_id\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:609\u001B[0m, in \u001B[0;36mBaseChatOpenAI._stream\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    607\u001B[0m     base_generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n\u001B[0;32m    608\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 609\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m response:\n\u001B[0;32m    611\u001B[0m     is_first_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    272\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    273\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:668\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    635\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    665\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    666\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m    667\u001B[0m     validate_response_format(response_format)\n\u001B[1;32m--> 668\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    680\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    682\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    683\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    684\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    685\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    686\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    687\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    688\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    689\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    692\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    693\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    694\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    695\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    696\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    697\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    698\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    699\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    700\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    701\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    703\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    704\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1260\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1246\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1248\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1255\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1256\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1257\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1258\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1259\u001B[0m     )\n\u001B[1;32m-> 1260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:937\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    929\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    930\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    935\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    936\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 937\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1041\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1038\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1040\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1044\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1045\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1049\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39moptions\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries,\n\u001B[0;32m   1050\u001B[0m )\n",
      "\u001B[1;31mAuthenticationError\u001B[0m: Error code: 401 - {'message': 'Unauthorized'}"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# ChatUpstage 객체를 생성합니다.\n",
    "upstage = ChatUpstage()\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = upstage.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41206a27",
   "metadata": {},
   "source": [
    "## Xionic\n",
    "\n",
    "사이오닉에이아이(Sionic AI)는 대한민국의 유망한 인공지능 스타트업으로, 기업용 생성형 AI 솔루션을 개발하고 있습니다. 다음은 이 회사에 대한 주요 정보입니다:\n",
    "\n",
    "### 주요 제품\n",
    "\n",
    "1. **STORM Platform**: 기업이 생성형 AI를 기술적 고민 없이 바로 적용할 수 있도록 하는 플랫폼\n",
    "2. **STORM Answer**: 기업에 최적화된 생성형 AI 솔루션으로 비즈니스 생산성 향상을 목표로 함\n",
    "3. **Xionic**: 상업적 이용이 가능한 라이센스의 한국어 AI 모델\n",
    "\n",
    "상업적 이용이 가능한 라이센스의 한국어 모델\n",
    "\n",
    "- 링크: https://github.com/sionic-ai/xionic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f39d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# xionic = ChatOpenAI(\n",
    "#     model_name=\"xionic-1-72b-20240610\",\n",
    "#     base_url=\"https://sionic.chat/v1/\",\n",
    "#     api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\n",
    "# )\n",
    "\n",
    "# 2024. 08. 06 llama3.1 업데이트\n",
    "xionic = ChatOpenAI(\n",
    "    model_name=\"llama-3.1-xionic-ko-70b\",\n",
    "    base_url=\"http://sionic.tech:28000/v1\",\n",
    "    api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = xionic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3eb61",
   "metadata": {},
   "source": [
    "# LogicKor\n",
    "\n",
    "LogicKor는 한국어 언어 모델의 다분야 사고력을 평가하기 위해 만들어진 벤치마크 리더보드입니다.\n",
    "\n",
    "## 목적\n",
    "\n",
    "한국어 언어 모델의 다양한 분야에서의 사고력을 측정하는 벤치마크\n",
    "\n",
    "## 평가 영역\n",
    "\n",
    "- 한국어 추론\n",
    "- 수학\n",
    "- 글쓰기\n",
    "- 코딩\n",
    "- 이해력\n",
    "\n",
    "## 주요 특징\n",
    "\n",
    "1. **다양한 모델 평가**: 국내외 다양한 언어 모델들의 성능을 비교할 수 있음\n",
    "2. **객관적 성능 측정**: 모델의 실제 성능을 다각도로 평가하여 객관적인 지표 제공\n",
    "3. **오픈 소스**: 누구나 접근하고 결과를 확인할 수 있는 오픈 플랫폼\n",
    "\n",
    "LogicKor 리더보드는 한국어 AI 모델의 발전을 위한 중요한 도구로 자리잡고 있으며, 지속적인 개선과 발전이 기대되고 있습니다.\n",
    "\n",
    "- 링크: [LogicKor 리더보드](https://lk.instruct.kr/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
