{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b099f91d",
   "metadata": {},
   "source": [
    "# Arxiv\n",
    "\n",
    "[arXiv](https://arxiv.org/)은 물리학, 수학, 컴퓨터 과학, 정량 생물학, 정량 금융, 통계, 전기공학 및 시스템 과학, 경제학 분야의 200만 편의 학술 논문을 위한 오픈 액세스 아카이브입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07926d71",
   "metadata": {},
   "source": [
    "[API 도큐먼트](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html#langchain_community.document_loaders.arxiv.ArxivLoader)\n",
    "\n",
    "Arxiv 문서 로더에 접근하려면 `arxiv`, `PyMuPDF` 및 `langchain-community` 통합 패키지를 설치해야 합니다. \n",
    "\n",
    "`PyMuPDF` 는 arxiv.org 사이트에서 다운로드한 PDF 파일을 텍스트 형식으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2694ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "# !pip install -qU langchain-community arxiv pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19348f",
   "metadata": {},
   "source": [
    "## 객체 생성\n",
    "\n",
    "이제 **model** 객체를 인스턴스화하고 문서를 로드할 수 있다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28d77a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T16:37:36.552268700Z",
     "start_time": "2024-10-05T16:37:35.901664700Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Query 에 검색하고자 하는 논문의 주제를 입력합니다.\n",
    "loader = ArxivLoader(\n",
    "    query=\"Chain of thought\",\n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=True,  # 메타데이터 전체 로드 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3037bd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T16:37:49.471206Z",
     "start_time": "2024-10-05T16:37:41.747892200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(metadata={'Published': '2023-11-15', 'Title': 'Contrastive Chain-of-Thought Prompting', 'Authors': 'Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing', 'Summary': 'Despite the success of chain of thought in enhancing language model\\nreasoning, the underlying process remains less well understood. Although\\nlogically sound reasoning appears inherently crucial for chain of thought,\\nprior studies surprisingly reveal minimal impact when using invalid\\ndemonstrations instead. Furthermore, the conventional chain of thought does not\\ninform language models on what mistakes to avoid, which potentially leads to\\nmore errors. Hence, inspired by how humans can learn from both positive and\\nnegative examples, we propose contrastive chain of thought to enhance language\\nmodel reasoning. Compared to the conventional chain of thought, our approach\\nprovides both valid and invalid reasoning demonstrations, to guide the model to\\nreason step-by-step while reducing reasoning mistakes. To improve\\ngeneralization, we introduce an automatic method to construct contrastive\\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\\ncontrastive chain of thought can serve as a general enhancement of\\nchain-of-thought prompting.', 'entry_id': 'http://arxiv.org/abs/2311.09277v1', 'published_first_time': '2023-11-15', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL'], 'links': ['http://arxiv.org/abs/2311.09277v1', 'http://arxiv.org/pdf/2311.09277v1']}, page_content='Contrastive Chain-of-Thought Prompting\\nYew Ken Chia∗1,\\nDeCLaRe\\nGuizhen Chen∗1, 2\\nLuu Anh Tuan2\\nSoujanya Poria\\nDeCLaRe\\nLidong Bing† 1\\n1DAMO Academy, Alibaba Group, Singapore\\nDeCLaRe\\nSingapore University of Technology and Design\\n2Nanyang Technological University, Singapore\\n{yewken_chia, sporia}@sutd.edu.sg\\n{guizhen001, anhtuan.luu}@ntu.edu.sg\\n{yewken.chia, guizhen.chen, l.bing}@alibaba-inc.com\\nAbstract\\nDespite the success of chain of thought in en-\\nhancing language model reasoning, the under-\\nlying process remains less well understood. Al-\\nthough logically sound reasoning appears in-\\nherently crucial for chain of thought, prior stud-\\nies surprisingly reveal minimal impact when\\nusing invalid demonstrations instead. Further-\\nmore, the conventional chain of thought does\\nnot inform language models on what mistakes\\nto avoid, which potentially leads to more er-\\nrors.\\nHence, inspired by how humans can\\nlearn from both positive and negative exam-\\nples, we propose contrastive chain of thought\\nto enhance language model reasoning. Com-\\npared to the conventional chain of thought, our\\napproach provides both valid and invalid rea-\\nsoning demonstrations, to guide the model to\\nreason step-by-step while reducing reasoning\\nmistakes. To improve generalization, we in-\\ntroduce an automatic method to construct con-\\ntrastive demonstrations. Our experiments on\\nreasoning benchmarks demonstrate that con-\\ntrastive chain of thought can serve as a general\\nenhancement of chain-of-thought prompting.1\\n1\\nIntroduction\\nWith the trend of large language models (LLMs),\\nmassively scaling the model size has enabled\\ngreater generalization (Brown et al., 2020) and\\nthe emergent ability to perform new tasks when\\ngiven suitable prompts (Wei et al., 2022a). How-\\never, solely increasing the model size cannot solve\\ncomplex reasoning tasks (Rae et al., 2022). To this\\nend, chain-of-thought prompting was proposed to\\nunlock the reasoning ability of LLMs by generating\\nintermediate reasoning steps (Wei et al., 2022b). In\\n∗Equal contribution. Yew Ken and Guizhen are students\\nunder the Joint PhD Program between Alibaba and their cor-\\nresponding university.\\n†Corresponding author.\\n1Our\\ncode\\nimplementation\\nwill\\nbe\\nreleased\\nat\\nhttps://github.com/DAMO-NLP-SG/contrastive-cot\\nExplanation: The dentist drills 4 teeth, so there are 30 - \\n4 = 26 teeth left. The dentist caps 7 more teeth than he \\ndrills, so he caps 4 + 7 = 11 teeth. Therefore, the \\ndentist fixes a total of 4 + 11 = 15 teeth. To find the \\npercentage of teeth the dentist fixes, we divide the \\nnumber of teeth fixed by the total number of teeth and \\nmultiply by 100: 15/30 x 100 = 50%\\nQuestion : James writes a 3-page letter to 2 different \\nfriends twice a week. How many pages does he write \\na year?\\nExplanation: He writes each friend 3*2=6 pages a \\nweek. So he writes 6*2=12 pages every week. That \\nmeans he writes 12*52=624 pages a year.\\nWrong Explanation: He writes each friend 12*52=624 \\npages a week. So he writes 3*2=6 pages every week. \\nThat means he writes 6*2=12 pages a year.\\nQuestion: James has 30 teeth. His dentist drills 4 of \\nthem and caps 7 more teeth than he drills. What \\npercentage of James\\' teeth does the dentist fix?\\nModel Input\\nModel Output\\nFigure 1: Example of contrastive chain-of-thought\\nwhich leverages both positive and negative demonstra-\\ntions to enhance language model reasoning.\\npractice, most methods based on chain of thought\\nleverage in-context learning (Brown et al., 2020)by\\nprompting the model with demonstrations of the in-\\nput, chain-of-thought, and output (Chu et al., 2023).\\nHowever, despite its success, we lack a thor-\\nough understanding of the chain of thought (Cooper\\net al., 2021). For example, it was shown that even\\ndemonstrations with invalid reasoning can lead to\\nsimilar performance compared to valid demonstra-\\ntions (Wang et al., 2023)2. Hence, it is not clear\\nhow language models learn to reason effectively\\nbased on the chain-of-thought demonstrations. On\\nthe other hand, mistakes in the intermediate steps\\ncan compound and derail the reasoning process\\n2Note that while chain-of-thought can be performed in\\na zero-shot fashion with prompts, we focus on the few-shot\\nsetting, as it was originally proposed in Wei et al. (2022b).\\narXiv:2311.09277v1  [cs.CL]  15 Nov 2023\\n(Ling et al., 2023). Any potential error in the rea-\\nsoning process not only affects the accuracy of the\\nfinal result but also undermines the trustworthiness\\nof the language model (Turpin et al., 2023). Thus,\\nit is also important to reduce mistakes in intermedi-\\nate reasoning steps.\\nTo address the challenges of chain of thought,\\nwe are inspired by how humans can learn from pos-\\nitive as well as negative examples. For instance,\\nwhen solving a complex task where the intermedi-\\nate steps are not well-defined, it is useful to learn\\nthe correct steps from positive demonstrations, as\\nwell as avoiding faults in negative demonstrations.\\nHence, we propose contrastive chain of thought,\\nwhich provides both positive and negative demon-\\nstrations to enhance the reasoning of language mod-\\nels. Naturally, this raises the question of how to\\ndesign effective negative demonstrations, as well\\nas whether they can be generalized to diverse tasks.\\nThrough our analysis of multiple invalid reasoning\\ntypes, we design a simple and effective method\\nthat can automatically generate contrastive demon-\\nstrations from existing valid reasoning chains. Fur-\\nthermore, as contrastive chain-of-thought is task-\\nagnostic and compatible with methods such as self-\\nconsistency (Wang et al., 2022), we believe that\\nit can serve as a general enhancement of chain of\\nthought.\\nTo measure the effectiveness of contrastive chain\\nof thought, we present evaluations on a wide range\\nof reasoning benchmarks, and find significant ben-\\nefits. Notably, compared to conventional chain of\\nthought, we observe improvements of 9.8 and 16.0\\npoints for GSM-8K (Cobbe et al., 2021) and Bam-\\nboogle (Press et al., 2023) respectively when using\\nGPT-3.5-Turbo3, a widely used LLM. Further anal-\\nysis of the reasoning chains generated from our\\nmethod also shows significant reduction in errors.\\nIn summary, our main contributions include: (1)\\nWe analyse various invalid reasoning types and\\nfind that combining positive and negative demon-\\nstrations generally boost the effectiveness of chain-\\nof-thought. (2) Based on the analysis above, we\\npropose contrastive chain of thought to enhance lan-\\nguage model reasoning. To improve generalization,\\nwe also propose an automatic method to construct\\ncontrastive demonstrations. (3) Evaluations on mul-\\ntiple reasoning benchmarks demonstrate significant\\nimprovements compared to conventional chain of\\nthought.\\n3https://platform.openai.com/docs/models\\n2\\nPreliminary Study: Effect of Different\\nTypes of Contrastive Demonstrations\\nWhile chain of thought (CoT) prompting has en-\\nhanced the reasoning of large language models, it\\nremains less well understood. For instance, while\\nsound reasoning seems intuitively important to ef-\\nfective chain of thought, previous work has shown\\nthat there is little effect when using invalid demon-\\nstrations. On the other hand, previous works in\\ncontrastive learning (Khosla et al., 2020) and align-\\nment (Ouyang et al., 2022) have demonstrated how\\nlanguage models can learn more effectively from\\nboth valid and invalid examples. Hence, we con-\\nduct a preliminary study with the following re-\\nsearch question: Can invalid reasoning demon-\\nstrations be instead used to enhance chain of\\nthought? Specifically, we aim to study the effect\\nof providing chain-of-thought demonstrations in a\\n“contrastive” manner, i.e., demonstrations contain-\\ning both valid and invalid rationales.\\n2.1\\nComponents of Chain of Thought\\nCompared to standard prompting with in-context\\ndemonstrations (Brown et al., 2020), chain-of-\\nthought (CoT) prompting (Wei et al., 2022b) in-\\ncludes a rationale for each demonstration example.\\nEach rationale consists of a series of intermedi-\\nate reasoning steps, guiding the language model to\\nsolve tasks in a step-by-step manner. Following the\\nformulation of (Wang et al., 2023), we identify two\\ndistinct components of each CoT rationale:\\n• Bridging objects are the symbolic items that\\nthe model traverses in order to reach the final\\nsolution. For example, the objects could be\\nnumbers and equations in arithmetic tasks, or\\nthe names of entities in factual tasks.\\n• Language templates are the textual hints that\\nguide the language model to derive and con-\\ntextualize the correct bridging objects during\\nthe reasoning process.\\n2.2\\nWhat is Invalid Chain of Thought?\\nGiven the distinct components of chain of thought,\\nwe are now able to systematically identify the as-\\npects which lead to invalid rationales. Concretely\\nthere are two main aspects which are applicable to\\nboth the language and object components:\\n• Coherence refers to the correct ordering of\\nsteps in a rationale, and is necessary for suc-\\ncessful chain of thought. Specifically, as chain\\nPrompting\\nMethod\\nArithmetic Reasoning Example\\nQuestion: Leah had 32 chocolates and her sister had 42. \\nIf they ate 35, how many pieces do they have left in total?\\nFactual Reasoning Example\\nQuestion: Who is the grandchild of Dambar Shah?\\nStandard\\nAnswer: 39\\nAnswer: Rudra Shah\\nChain-of-Thought \\n(CoT)\\nStep-by-step answer: Originally, Leah had 32 chocolates \\nand her sister had 42. So in total they had 32 + 42 = 74. \\nAfter eating 35, they had 74 - 35 = 39 pieces left in total. \\nThe answer is 39.\\nStep-by-step answer: Dambar Shah (? - 1645) was the \\nfather of Krishna Shah. Rudra Shah was the child of \\nKrishna Shah (? - 1661). So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Invalid \\nReasoning\\nStep-by-step answer: Originally, Leah had 32 chocolates \\nand her sister had 42. So her sister had 42 - 32 = 10 \\nchocolates more than Leah has. After eating 35, since 10 \\n+ 35 = 45, they had 45 - 6 = 39 pieces left in total. The \\nanswer is 39. \\nStep-by-step answer: Dambar Shah (? - 1645) was the \\nking of the Gorkha Kingdom. The Gorkha Kingdom was \\nestablished by Prince Dravya Shah. Dravya Shah has a \\nchild named Rudra Shah. So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Incoherent \\nObjects\\nStep-by-step answer: Originally, Leah had 32 + 42 = 74 \\nchocolates and her sister had 32. So in total they had 74 - \\n35 = 39. After eating 35, they had 42 pieces left in total. \\nThe answer is 39.\\nStep-by-step answer: Krishna Shah was the father of \\nRudra Shah. Dambar Shah (? - 1645) was the child of \\nKrishna Shah (? - 1661). So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Incoherent \\nLanguage\\nStep-by-step answer: After eating 32, they had 42 pieces \\nleft in total. Originally, Leah had 32 + 42 = 74 chocolates \\nand her sister had 35. So in total they had 74 - 35 = 39. \\nThe answer is 39.\\nStep-by-step answer: Dambar Shah (? - 1645) was the \\nchild of Krishna Shah. Rudra Shah was the father of \\nKrishna Shah (? - 1661). So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Irrelevant \\nObjects\\nStep-by-step answer: Originally, Leah had 19 chocolates \\nand her sister had 31. So in total they had 19 + 31 = 50. \\nAfter eating 29, they had 50 - 29 = 21 pieces left in total. \\nThe answer is 21.\\nStep-by-step answer: Metis Amando was the father of \\nDavid Amando. Randall Amando was the child of David \\nAmando. So the final answer (the name of the \\ngrandchild) is: Randall Amando.\\nCoT: Irrelevant \\nLanguage\\nStep-by-step answer: Patricia needs to donate 32 inches, \\nand wants her hair to be 42 inches long after the donation. \\nHer hair is 35 inches long currently. Her hair needs to be \\n32 + 42 = 74 inches long when she cuts it. So she needs \\nto grow 74 - 35 = 39 more inches. The answer is 39.\\nStep-by-step answer: The husband of Dambar Shah (? - \\n1645) is Krishna Shah. Krishna Shah (? - 1661) has a \\nbrother called Rudra Shah. So the final answer (the \\nname of the brother-in-law) is: Rudra Shah.\\nLanguage Component\\nObject Component\\nInvalid Component (Reasoning / Language / Object)\\nFigure 2: Categorization of invalid chain-of-thought examples, following Wang et al. (2023).\\nof thought is a sequential reasoning process,\\nit is not possible for later steps to be pre-\\nconditions of earlier steps.\\n• Relevance refers to whether the rationale\\ncontains corresponding information from the\\nquestion. For instance, if the question men-\\ntions a person named Leah eating chocolates,\\nit would be irrelevant to discuss a different\\nperson cutting their hair.\\nIn addition, following Wang et al. (2023), we\\ninclude invalid reasoning as a category of invalid\\nchain of thought, which is neither incoherent nor\\nirrelevant, but contains logical mistakes. Hence,\\nwe aim to study the five main categories of invalid\\nchain-of-thought, as shown in Figure 2.\\n2.3\\nExperimental Setup\\nTo conduct the experiments for the preliminary\\nstudy, we leverage the GSM8K (Cobbe et al., 2021)\\nand Bamboogle (Press et al., 2023) datasets for\\narithmetic and factual reasoning respectively. We\\nuse the OpenAI Chat Completions API4 which is\\none of the most popular and well-performing lan-\\nguage models with reasonable cost. Specifically,\\nwe use the GPT-3.5-Turbo (0301) version. To study\\nthe effect of contrastive demonstrations under vari-\\nous settings, we evaluate the five main invalid cat-\\negories as shown in Figure 2. Note that we use\\n4-shot prompting for each dataset, and the chain-of-\\nthought demonstrations are manually constructed\\nby previous works (Wei et al., 2022b; Wang et al.,\\n2023). To standardize the prompting process, we\\nuse a simplified chain-of-thought prompt format,\\nas shown in Figure 1.\\n2.4\\nPreliminary Results\\nBased on the preliminary results in Table 1, we\\nobserve significant gains across all invalid ratio-\\nnale categories compared to conventional chain-\\nof-thought. Notably, leveraging chain of thought\\nwith contrastive demonstrations containing incoher-\\nent objects yields the highest average performance\\n4https://platform.openai.com/docs/api-reference\\nAnswer: 37.5%\\nExplanation: The dentist fixes a total of \\n4 + 7 = 11 teeth. To find the \\npercentage, we divide the number of \\nteeth fixed by the total number of teeth \\nand multiply by 100: 11/30 x 100 = \\n36.67%\\nExplanation: The dentist drills 4 teeth, so there \\nare 30 - 4 = 26 teeth left. The dentist caps 7 \\nmore teeth than he drills, so he caps 4 + 7 = 11 \\nteeth. Therefore, the dentist fixes a total of 4 + 11 \\n= 15 teeth. To find the percentage of teeth the \\ndentist fixes, we divide the number of teeth fixed \\nby the total number of teeth and multiply by 100: \\n15/30 x 100 = 50%\\nQuestion : James writes a 3-page letter \\nto 2 different friends twice a week. How \\nmany pages does he write a year?\\nExplanation: He writes each friend \\n3*2=6 pages a week So he writes \\n6*2=12 pages every week. That means \\nhe writes 12*52=624 pages a year.\\nQuestion: James has 30 teeth. His \\ndentist drills 4 of them and caps 7 more \\nteeth than he drills. What percentage of \\nJames\\' teeth does the dentist fix?\\nQuestion : James writes a 3-page letter to 2 \\ndifferent friends twice a week. How many pages \\ndoes he write a year?\\nExplanation: He writes each friend 3*2=6 pages \\na week. So he writes 6*2=12 pages every week. \\nThat means he writes 12*52=624 pages a year.\\nWrong Explanation: He writes each friend \\n12*52=624 pages a week. So he writes 3*2=6 \\npages every week. That means he writes 6*2=12 \\npages a year.\\nQuestion: James has 30 teeth. His dentist drills 4 \\nof them and caps 7 more teeth than he drills. \\nWhat percentage of James\\' teeth does the \\ndentist fix?\\nQuestion: James writes a \\n3-page letter to 2 different \\nfriends twice a week. How \\nmany pages does he write \\na year?\\nAnswer: 624\\nQuestion: James has 30 \\nteeth. His dentist drills 4 of \\nthem and caps 7 more \\nteeth than he drills. What \\npercentage of James\\' teeth \\ndoes the dentist fix?\\nChain-of-Thought (CoT)\\nContrastive Chain-of-Thought\\nStandard Prompting\\nModel Input\\nModel Input\\nModel Input\\nModel Output\\nModel Output\\nModel Output\\nFigure 3: Overview of contrastive chain-of-thought (right), with comparison to common prompting methods.\\nPrompting Method\\nGSM8K\\nBamboogle\\nAvg.\\nStandard\\n27.4\\n11.2\\n19.3\\nChain-of-Thought\\n69.2\\n40.8\\n55.0\\nw/ Invalid Reasoning\\n76.0\\n45.6\\n60.8\\nw/ Incoherent Objects\\n79.6\\n53.6\\n66.6\\nw/ Incoherent Language\\n78.8\\n52.8\\n65.8\\nw/ Irrelevant Objects\\n79.8\\n48.8\\n64.3\\nw/ Irrelevant Language\\n80.2\\n49.6\\n64.9\\nTable 1: Preliminary results on the effect of contrastive\\ndemonstrations for chain of thought.\\non GSM8K and Bamboogle. This suggests that\\nlanguage models are better able to learning step-\\nby-step reasoning when provided with both valid\\nand invalid rationales. Hence, we believe that con-\\ntrastive demonstrations have the potential to greatly\\nenhance language model reasoning ability.\\n3\\nContrastive Chain of Thought\\nChain-of-thought (CoT) prompting, as evidenced\\nby prior research, has indeed elevated the reasoning\\ncapabilities of large language models (Wei et al.,\\n2022b). However, a comprehensive understand-\\ning of this phenomenon is still lacking. Although\\nlogically sound reasoning appears to be inherently\\ncrucial for chain of thought, prior studies surpris-\\ningly reveal minimal impact when employing in-\\nvalid demonstrations. To this end, based on our\\npreliminary study in Section 2, we found that pro-\\nviding both valid and invalid reasoning demonstra-\\ntions in a “contrastive” manner greatly improves\\nreasoning performance. However, this approach\\nmay not generalize well to new tasks, as it requires\\nmanual construction of the invalid rationales.\\nThus, we propose a general prompting method\\nknown as contrastive chain of thought, which\\nincludes automatic construction of contrastive\\ndemonstrations. Figure 3 presents an overview of\\nour approach. Specifically, the language model is\\nprovided with the question, ground truth answer ex-\\nplanation and incorrect answer explanation. Com-\\npared to standard prompting, our method enables\\nmodels to perform more complex reasoning by de-\\ncomposing problems into intermediate steps. Com-\\npared to conventional chain-of-thought prompting,\\nour method contrasts the valid and invalid answer\\nexplanations, guiding the model to generate more\\naccurate reasoning chains.\\nConcretely, given a small set of n in-context\\ndemonstration examples D = {E1, . . . , E|n|}, and\\na query Q, the goal of the model is to generate a\\nsuitable answer A. For standard prompting, the\\ndemonstration examples consist of just the ques-\\ntion and answer, i.e., Ej = (Qj, Aj). On the other\\nhand, chain-of-thought is a more advanced prompt-\\ning method that guides the model with intermediate\\nPrompting Method\\nArithmetic Reasoning\\nFactual QA\\nGSM8K\\nAQuA\\nGSM-Hard\\nSVAMP\\nASDIV\\nBamboogle\\nStrategyQA\\nStandard\\n27.4\\n29.5\\n11.2\\n69.3\\n75.8\\n12.0\\n59.4\\nCoT\\n69.2\\n53.5\\n33.8\\n67.2\\n70.8\\n40.8\\n55.8\\nContrastive CoT\\n79.0 (+9.8)\\n57.5 (+3.9)\\n44.2 (+10.4)\\n81.6 (+14.4)\\n84.4 (+13.6)\\n56.8 (+16.0)\\n66.2 (+10.4)\\nStandard-SC\\n28.0\\n29.9\\n11.0\\n69.0\\n76.0\\n11.2\\n59.6\\nCoT-SC\\n71.0\\n55.9\\n34.0\\n71.6\\n74.0\\n40.8\\n57.0\\nContrastive CoT-SC\\n86.2 (+15.2)\\n71.7 (+15.7)\\n50.0 (+16.0)\\n85.2 (+13.6)\\n89.6 (+15.6)\\n58.4 (+17.6)\\n69.6 (+12.6)\\nTable 2: Main evaluation results for contrastive chain-of-thought on several reasoning tasks.\\nDataset\\nType\\n|Train|\\n|Test|\\nGSM8K\\nArithmetic Reasoning\\n4\\n500\\nAQuA\\nArithmetic Reasoning\\n4\\n254\\nGSM-Hard\\nArithmetic Reasoning\\n4\\n500\\nSVAMP\\nArithmetic Reasoning\\n4\\n500\\nASDIV\\nArithmetic Reasoning\\n4\\n500\\nBamboogle\\nFactual QA\\n4\\n125\\nStrategyQA\\nFactual QA\\n4\\n500\\nTable 3: Details of datasets used.\\nreasoning steps T. As shown in the figure above,\\nthe reasoning steps T typically consist of multi-\\nple sentences where each sentence describes one\\nreasoning step. Hence, chain-of-thought prompt-\\ning examples consist of the question, reasoning\\nsteps, and final answer, i.e., Ej = (Qj, Tj, Aj).\\nHowever, the model does not know what faults\\nto avoid in conventional chain-of-thought, which\\ncould lead to increased mistakes and error prop-\\nagation. Hence, our contrastive chain of thought\\nmethod provides both the correct and incorrect rea-\\nsoning steps in the demonstration examples, i.e.,\\nEj = (Qj, Tj,+, Aj,+, Tj,−, Aj,−).\\nTo obtain the correct reasoning steps T+ for the\\ndemonstration examples, we use the annotated ex-\\namples from the previous chain-of-thought works.\\nFor the incorrect reasoning steps T−, we automati-\\ncally construct it from the correct reasoning steps\\nT+, based on the \"Incoherent Objects\" category in\\nSection 2. Concretely, we use an existing entity\\nrecognition model5 to extract the object spans such\\nas numbers, equations, or persons from a given\\nchain-of-thought rationale. Consequently, we ran-\\ndomly shuffle the position of the objects within the\\nrationale, thus constructing a rationale with inco-\\nherent bridging objects. Note that when testing\\nwith a new question, only the question and demon-\\nstration examples are provided to the model, and\\nthe model must generate its own reasoning steps\\n5https://spacy.io/models/en#en_core_web_trf\\nbefore producing the final answer.\\n4\\nExperiments\\n4.1\\nExperimental Setup\\nWe focus our study on two main types of reasoning\\ntasks: arithmetic reasoning and factual question\\nanswering (QA). For arithmetic reasoning, we con-\\nduct experiments on a range of datasets including\\nGSM8K (Cobbe et al., 2021), AQuA (Ling et al.,\\n2017), GSM-Hard (Gao et al., 2023), SVAMP (Pa-\\ntel et al., 2021), and ASDIV (Miao et al., 2020).\\nFor factual QA, we include two datasets: Bam-\\nboogle (Press et al., 2023) and StrategyQA (Geva\\net al., 2021). To maintain a reasonable computing\\nbudget, we limit each dataset to a maximum of\\n500 test samples through random sampling. For\\ndatasets that contain less than 500 test samples, we\\ninstead use all available test samples. The datasets’\\ndetails are included in Table 3. Regarding model\\nand prompting details, we use the same experimen-\\ntal setup as for our preliminary study in Section\\n2.\\n4.2\\nMain Results\\nTo assess the effectiveness of our method, we eval-\\nuate on several reasoning tasks and report the main\\nresults in Table 2. Our main findings are as follows:\\nContrastive CoT demonstrates consistent im-\\nprovements\\nover\\nconventional\\nCoT.\\nCon-\\ntrastive CoT consistently outperforms conventional\\nCoT across the datasets in both arithmetic and fac-\\ntual reasoning categories. Notably, we observe\\nsubstantial gains of more than 10 points on GSM-\\nHard, SVAMP, ASDIV, Bamboogle and Strate-\\ngyQA. Thus, the consistent and significant perfor-\\nmance improvements demonstrate the general ef-\\nfectiveness of our proposed method. As contrastive\\nchain of thought can be automatically constructed\\nfrom existing rationales, the annotation cost is the\\nsame as conventional chain of thought. Hence, it\\ncan be viewed as a general enhancement of chain\\nof thought.\\nContrastive CoT is more effective when ap-\\nplied with self-consistency.\\nAs self-consistency\\n(Wang et al., 2022) is a popular decoding strategy\\nto boost the chain-of-thought performance of large\\nlanguage models, we are interested to see if con-\\ntrastive chain of thought can benefit similarly from\\nself-consistency. In general, we observe that self-\\nconsistency further enhances the performance of\\ncontrastive CoT. This enhancement is particularly\\nevident in the case of the AQuA dataset. While con-\\ntrastive CoT alone results in a modest performance\\nimprovement of 4.0%, applying self-consistency\\namplifies this gain significantly, achieving an addi-\\ntional improvement of 14.2%.\\n5\\nRelated Work\\nLarge Language Models\\nRecent developments\\nin large language models have shown that mas-\\nsively scaling the size and training data of models\\ncan greatly improve generalization (Kaplan et al.,\\n2020). Notably, large language models have been\\nshown to generalize to new tasks when given suit-\\nable prompts and demonstrations (Brown et al.,\\n2020). This has brought about a new paradigm of\\nleveraging language models for tasks without the\\nneed for additional training (Liu et al., 2023). How-\\never, simply scaling language models has not been\\nsufficient to attain good performance on challeng-\\ning tasks such as arithmetic reasoning and factual\\nquestion answering (Wei et al., 2022b). Hence, in\\nthis work, we focus on enhancing the reasoning\\nability of large language models through prompts.\\nChain of Thought\\nChain-of-thought prompting\\nwas introduced by Wei et al. (2022b) to enhance\\nlanguage model reasoning by generating interme-\\ndiate steps. Notably, this has inspired numerous\\nworks that build upon this direction of step-by-\\nstep reasoning. For instance, automatic chain-of-\\nthought (Zhang et al., 2023) was proposed to ad-\\ndress the challenges in manually annotating chain-\\nof-thought demonstrations. On the other hand, it\\nwas shown that specific prompts such as “Let’s\\nthink step-by-step” can enable language models\\nto perform chain-of-thought in a zero-shot man-\\nner, without any demonstrations (Kojima et al.,\\n2022). In addition, challenging problems can be de-\\ncomposed into multiple sub-problems (Zhou et al.,\\n2023), or even into code programs that can be au-\\ntomatically executed (Gao et al., 2023). Despite\\nthe progress in chain-of-thought on multiple fronts,\\nwe still lack a rigorous understanding of the under-\\nlying mechanism (Turpin et al., 2023; Feng et al.,\\n2023). In this work, inspired by the findings of pre-\\nvious works regarding invalid demonstrations, we\\npropose contrastive chain-of-thought to enhance\\nlanguage model reasoning. As contrastive chain-\\nof-thought leverages both valid and invalid reason-\\ning demonstrations, we believe this may encour-\\nage other researchers to fundamentally rethink the\\nchain-of-thought process.\\nLearning from Negative Examples\\nWhile\\nchain-of-thought prompting typically involves only\\nvalid demonstrations, it is not clear whether in-\\nvalid demonstrations can also benefit the reason-\\ning process (Wang et al., 2023).\\nOn the other\\nhand, learning from negative or invalid samples\\nis not new. For instance, contrastive learning is\\na well-established deep learning approach that en-\\ncourages models to distinguish between “positive”\\nand “negative” samples, thus learning better rep-\\nresentations (Khosla et al., 2020). Similarly, rein-\\nforcement learning from human feedback (RLHF)\\ntrains a reward model based on positive and neg-\\native samples of human preference data (Ouyang\\net al., 2022; Christiano et al., 2017). Hence, in-\\nspired by the previous approaches, we propose con-\\ntrastive chain-of-thought, a general enhancement\\nof chain-of-thought prompting, by enabling mod-\\nels to learn from both valid and invalid reasoning\\ndemonstrations.\\n6\\nConclusions\\nIn this work, we have explored the effect of leverag-\\ning invalid reasoning demonstrations for enhancing\\nchain of thought. Through our preliminary study\\non different invalid chain-of-thought categories, we\\nfound that providing both valid and invalid demon-\\nstrations in a contrastive manner greatly improves\\nreasoning ability in language models. To overcome\\nthe challenge of manually annotating invalid ratio-\\nnales, we propose contrastive chain of thought, a\\ngeneral prompting method which can automatically\\nconstruct contrastive demonstrations from existing\\nrationales. Through experiments on several reason-\\ning tasks, we find contrastive chain of thought to be\\na general enhancement of chain-of-thought prompt-\\ning. Further investigation into alternative forms of\\nchain-of-thought prompting will hopefully inspire\\nfuture advancements in language-based reasoning.\\nReferences\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nClark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners.\\nIn Ad-\\nvances in Neural Information Processing Systems,\\nvolume 33, pages 1877–1901. Curran Associates,\\nInc.\\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\\ntic, Shane Legg, and Dario Amodei. 2017. Deep\\nreinforcement learning from human preferences. In\\nAdvances in Neural Information Processing Systems,\\nvolume 30. Curran Associates, Inc.\\nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang\\nYu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,\\nBing Qin, and Ting Liu. 2023. A survey of chain of\\nthought reasoning: Advances, frontiers and future.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\\nNakano, Christopher Hesse, and John Schulman.\\n2021. Training verifiers to solve math word prob-\\nlems. CoRR, abs/2110.14168.\\nNathan Cooper, Carlos Bernal-Cárdenas, Oscar Cha-\\nparro, Kevin Moran, and Denys Poshyvanyk. 2021.\\nIt takes two to tango: Combining visual and textual\\ninformation for detecting duplicate video-based bug\\nreports. CoRR, abs/2101.09194.\\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye,\\nDi He, and Liwei Wang. 2023. Towards revealing\\nthe mystery behind chain of thought: A theoretical\\nperspective. In Thirty-seventh Conference on Neural\\nInformation Processing Systems.\\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\\nham Neubig. 2023. PAL: Program-aided language\\nmodels. In Proceedings of the 40th International\\nConference on Machine Learning, volume 202 of\\nProceedings of Machine Learning Research, pages\\n10764–10799. PMLR.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 9:346–\\n361.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\\nScaling laws for neural language models. CoRR,\\nabs/2001.08361.\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\\nSarna,\\nYonglong\\nTian,\\nPhillip\\nIsola,\\nAaron\\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\\npervised contrastive learning. In Advances in Neural\\nInformation Processing Systems, volume 33, pages\\n18661–18673. Curran Associates, Inc.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. In Advances\\nin Neural Information Processing Systems.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 158–167, Vancouver,\\nCanada. Association for Computational Linguistics.\\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang,\\nMingu Lee, Roland Memisevic, and Hao Su. 2023.\\nDeductive verification of chain-of-thought reasoning.\\nIn Thirty-seventh Conference on Neural Information\\nProcessing Systems.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\\ntrain, prompt, and predict: A systematic survey of\\nprompting methods in natural language processing.\\nACM Comput. Surv., 55(9).\\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\\n2020. A diverse corpus for evaluating and developing\\nEnglish math word problem solvers. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 975–984, Online.\\nAssociation for Computational Linguistics.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Gray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. In Advances in Neural Information\\nProcessing Systems.\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\\n2021. Are NLP models really able to solve simple\\nmath word problems? In Proceedings of the 2021\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, pages 2080–2094, Online.\\nAssociation for Computational Linguistics.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A. Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language\\nmodels.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\\nMillican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susan-\\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\\ncob Menick, Albin Cassirer, Richard Powell, George\\nvan den Driessche, Lisa Anne Hendricks, Mari-\\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\\nAidan Clark, Diego de Las Casas, Aurelia Guy,\\nChris Jones, James Bradbury, Matthew Johnson,\\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\\nlanguage models: Methods, analysis & insights from\\ntraining gopher.\\nMiles Turpin, Julian Michael, Ethan Perez, and\\nSamuel R. Bowman. 2023. Language models don’t\\nalways say what they think: Unfaithful explanations\\nin chain-of-thought prompting. In Thirty-seventh\\nConference on Neural Information Processing Sys-\\ntems.\\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2023.\\nTowards understanding chain-of-thought prompting:\\nAn empirical study of what matters. In Proceedings\\nof the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 2717–2739, Toronto, Canada. Association for\\nComputational Linguistics.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Huai hsin Chi, and Denny Zhou. 2022. Self-\\nconsistency improves chain of thought reasoning in\\nlanguage models. ArXiv, abs/2203.11171.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\\n2022a. Emergent abilities of large language models.\\nTrans. Mach. Learn. Res., 2022.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022b. Chain of thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2023. Automatic chain of thought prompting\\nin large language models. In The Eleventh Interna-\\ntional Conference on Learning Representations.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H.\\nChi. 2023. Least-to-most prompting enables com-\\nplex reasoning in large language models. In The\\nEleventh International Conference on Learning Rep-\\nresentations.\\n'),\n Document(metadata={'Published': '2024-03-23', 'Title': 'Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models', 'Authors': 'Yao Yao, Zuchao Li, Hai Zhao', 'Summary': \"With the widespread use of language models (LMs) in NLP tasks, researchers\\nhave discovered the potential of Chain-of-thought (CoT) to assist LMs in\\naccomplishing complex reasoning tasks by generating intermediate steps.\\nHowever, human thought processes are often non-linear, rather than simply\\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\\nreasoning, which models human thought processes not only as a chain but also as\\na graph. By representing thought units as nodes and connections between them as\\nedges, our approach captures the non-sequential nature of human thinking and\\nallows for a more realistic modeling of thought processes. GoT adopts a\\ntwo-stage framework with an additional GoT encoder for thought graph\\nrepresentation and fuses the graph representation with the original input\\nrepresentation through a gated fusion mechanism. We evaluate GoT's performance\\non a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task\\n(ScienceQA). Our model achieves significant improvement over the strong CoT\\nbaseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59%\\nusing the T5-base model over the state-of-the-art Multimodal-CoT on the\\nScienceQA test set.\", 'entry_id': 'http://arxiv.org/abs/2305.16582v2', 'published_first_time': '2023-05-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL'], 'links': ['http://arxiv.org/abs/2305.16582v2', 'http://arxiv.org/pdf/2305.16582v2']}, page_content='Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\\nLanguage Models\\nYao Yao1,2, Zuchao Li3,∗and Hai Zhao1,2,∗\\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\\n2MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\\n3National Engineering Research Center for Multimedia Software,\\nSchool of Computer Science, Wuhan University, Wuhan, 430072, P. R. China\\nyaoyao27@sjtu.edu.cn, zcli-charlie@whu.edu.cn,\\nzhaohai@cs.sjtu.edu.cn\\nAbstract\\nWith the widespread use of language mod-\\nels (LMs) in NLP tasks, researchers have\\ndiscovered the potential of Chain-of-thought\\n(CoT) to assist LMs in accomplishing com-\\nplex reasoning tasks by generating intermedi-\\nate steps. However, human thought processes\\nare often non-linear, rather than simply se-\\nquential chains of thoughts.\\nTherefore, we\\npropose Graph-of-Thought (GoT) reasoning,\\nwhich models human thought processes not\\nonly as a chain but also as a graph. By repre-\\nsenting thought units as nodes and connections\\nbetween them as edges, our approach captures\\nthe non-sequential nature of human thinking\\nand allows for a more realistic modeling of\\nthought processes. GoT adopts a two-stage\\nframework with an additional GoT encoder\\nfor thought graph representation and fuses the\\ngraph representation with the original input\\nrepresentation through a gated fusion mech-\\nanism. We evaluate GoT’s performance on a\\ntext-only reasoning task (AQUA-RAT) and a\\nmultimodal reasoning task (ScienceQA). Our\\nmodel achieves significant improvement over\\nthe strong CoT baseline on the AQUA-RAT test\\nset and boosts accuracy from 85.19% to 87.59%\\nusing the T5-base model over the state-of-the-\\nart Multimodal-CoT (Zhang et al., 2023) on the\\nScienceQA test set. Our code is publicly avail-\\nable at https://github.com/Zoeyyao27/Graph-\\nof-Thought\\n1\\nIntroduction\\nIn the field of human cognition, it has long been rec-\\nognized that the human thought process is far more\\ncomplex and non-linear than could be captured by\\na simple, sequential chain of thoughts (Barsalou,\\n∗Corresponding author. This research was supported\\nby the National Natural Science Foundation of China (No.\\n62306216), the Natural Science Foundation of Hubei Province\\nof China (No. 2023AFB816), the Fundamental Research\\nFunds for the Central Universities (No. 2042023kf0133), the\\nJoint Research Project of Yangtze River Delta Science and\\nTechnology Innovation Community (No. 2022CSJGG1400).\\n1999). Human thinking is often characterized by\\nits ability to make sudden leaps and connections\\nbetween seemingly unrelated ideas, which can lead\\nto novel insights and solutions. This non-linear,\\njumping thought process is a hallmark of human\\ncreativity, reasoning, and problem-solving abilities.\\nHowever, it also poses a significant challenge for\\ncognitive modeling and understanding.\\nRecently, Large Language Models (LLMs) have\\nbeen advancing at an unprecedented pace. With\\nthe emergence of breakthroughs such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022), and GPT-4 (OpenAI, 2023), the field of\\nnatural language processing has entered a new\\nera of possibilities. Recent studies (Wei et al.,\\n2022a; Wang et al., 2022; Zhang et al., 2022) have\\nshown that the reasoning ability of LLMs can be\\nunlocked by Chain-of-Thought (CoT) prompting.\\nCoT prompting involves a series of intermediate\\nnatural language rationales that lead to the final\\nanswer. In addition, Zhang et al. (2023) have in-\\ntroduced Multimodal-CoT, which combines both\\nlanguage and visual modalities to help surpass the\\nlimitations of textual information. More detailed\\nrelated works can be found in Appendix A.\\nPrevious works on Chain-of-Thought (CoT)\\nprompting, which have been limited to textual and\\nvisual information, often represented the human\\nreasoning process as sequential thought chains.\\nThis approach overlooks the modeling of humans’\\njumping thought process and neglects to incorpo-\\nrate the complex structural information of reason-\\ning thoughts into the model.\\nConcurrent work\\nTree-of-thoughts (ToT) (Yao et al., 2023) divides\\nthoughts into thought units and models them as a\\ntree-like search process.\\nNevertheless, human cognition transcends this\\ntree structure, exhibiting intricate graph-like for-\\nmations. Our perspective diverges further as we\\nbelieve that the human intellect is capable of craft-\\ning elaborate thought graphs founded upon linear\\narXiv:2305.16582v2  [cs.CL]  23 Mar 2024\\nDo ferns produce seeds?\\nText Features\\n(A) Yes\\n(B) No\\nThis diagram shows the life cycle of \\na fern.\\nVision Features (Optional)\\nGraph-of-Thought Features\\nproduce\\nseeds\\nferns\\nshows\\nlife \\ncycle\\nof\\ndiagram\\nFern plants reproduce using both asexual reproduction \\nand sexual reproduction … The heart-shaped plant \\nbegins the fern\\'s sexual reproduction stage … The mature \\nfern can make spores and begin the fern life cycle again.\\nRationale\\nFerns do not produce seeds. Mature ferns produce spores, \\nand heart-shaped plants produce eggs and sperm.\\nAnswer\\nThe answer \\nis (B)\\nGraph-of-Thought with Rationale\\nproduce\\nseeds\\nferns\\nshows\\nlife \\ncycle\\nof\\ndiagram\\nhas\\nsexual \\nproduction\\nstage\\nFigure 1: An example of GoT reasoning. Vision features are optional and are only required in multimodal reasoning.\\nthoughts. Therefore, we aim to enable the concur-\\nrent assimilation of linear and nonlinear cognitive\\nprocesses, surpassing the mere generation of seg-\\nmented thought units. To address the above limi-\\ntation, different from ToT, we propose the Graph-\\nof-Thought (GoT), a novel approach to modeling\\nhuman thought processes not only as a chain but\\nalso as a graph. Our method is based on the assump-\\ntion that the human mind works by connecting and\\nrecombining ideas in a non-sequential, graph fash-\\nion, rather than following a strict sequential chain.\\nBy representing thought units as nodes and connec-\\ntions between thoughts as edges, GoT captures the\\nrich, non-sequential nature of human thinking and\\nallows for a more realistic and logical modeling of\\nreasoning processes.\\nAn example of GoT reasoning is shown in Fig-\\nure 1. Inspired by Multimodal-CoT (Zhang et al.,\\n2023), we have adopted a two-stage reasoning\\nframework. It first generates rationales and then\\ngenerates the final answer based on the predicted ra-\\ntionales. In addition to text features, graph features\\nof GoT are integrated during the rationale genera-\\ntion and answer inference. Specifically, GoT is first\\nconstructed with an Extract-Cluster-Coreference\\n(ECC) process, which simulates the deductive pro-\\ncess in human reasoning. We have used T5 (Raffel\\net al., 2020a) pre-trained language model as our\\nbackbone model. GoT is encoded with a graph\\nattention network and then fused with the original\\nrepresentation via a gated fusion network.\\nFurthermore, we have also presented a multi-\\nmodal GoT, which integrates not only text features\\nand GoT features but also visual features. For our\\nexperiments, we have used both FLAN-Alpaca\\n1 (T5)-base and FLAN-Alpaca (T5)-large as our\\nbackbone models.\\nWe implement GoT as a two-stage framework\\nand fine-tuning language models and integrating\\ntext, thought graph, and vision features for a more\\nrealistic and accurate reasoning process.\\nGoT\\ndemonstrates exceptional performance on both text-\\nonly AQUA-RAT (Ling et al., 2017) and multi-\\nmodal ScienceQA (Lu et al., 2022) benchmarks,\\nsurpassing the accuracy of online system Chat-\\nGPT (OpenAI, 2023) by 9.28%, strong baseline\\nMultimodal-CoT (Zhang et al., 2023) by 2.40%,\\nand even exceeding human performance, establish-\\ning a new state-of-the-art on ScienceQA test set\\nwith far fewer parameters.\\n2\\nGraph-of-Thought\\nThe overview of our proposed GoT can be seen\\nin Figure 2. Inspired by Multimodal-CoT (Zhang\\net al., 2023), GoT also adopts a two-stage frame-\\nwork. (1) Rationale generation stage: In the first\\nstage, the model generates rationales based on the\\ninput text (including question, context, and choices)\\nthe vision features, and the generated thought graph\\ncorresponding to the input text. For multi-modal\\ntasks (Zhang et al., 2023; Zhang and Zhang, 2023;\\nHuang et al., 2023; Peng et al., 2023), it is a com-\\nmon practice to use different encoders to process\\ninputs from different modalities and a straightfor-\\n1https://github.com/declare-lab/flan-alpaca. FLAN-Alpaca\\nis developed by fine-tuning T5 model on the Flan collection\\nThought Graph\\nImage (Optional)\\nGraph-of-Thought \\nConstructor\\nInput Text \\nQuestion: Do ferns \\nproduce seeds?\\nChoices: (A) Yes (B) No\\nContext: This diagram \\nshows the life cycle of \\na fern.\\nPredicted \\nRationales\\nInput \\nEncoder\\nGoT\\nEncoder\\nText\\nencoder\\nVision \\nencoder\\nGraph \\nAttention \\nNetwork \\nTransformer\\nEncoder\\nFeature\\nExtractor\\nCross\\nAttention\\nCross\\nAttention\\nGated\\nFusion\\nLayer\\nTransformer\\nDecoder\\nStage 1\\nPredict Rationales\\nLecture：Fern plants reproduce \\nusing both asexual reproduction \\nand sexual reproduction…\\nSolution:  Ferns do not produce \\nseeds. Mature ferns produce \\nspores…\\nThe answer is (B).\\nDecoder\\nOutput \\nFeature Fusion\\nStage 2\\nStage 2\\nPredict Answers\\nFigure 2: Graph-of-Thought framework overview\\nward and versatile approach is to employ encoder-\\ndecoder models. Therefore, GoT employs inde-\\npendent encoders to encode input data for each\\nmodality. We use a Transformer encoder to encode\\ninput text, a vision encoder to encode an image, and\\na graph attention network to encode the thought\\ngraph. The encoded features are further passed\\ninto cross-attention to align text tokens with image\\npatches and graph nodes, respectively. We then\\nuse a gated fusion layer to fuse these three features\\nfurther and pass them into the Transformer decoder\\nto predict the target rationales. (2) Answer gener-\\nation stage: The second stage aims at generating\\nthe final answer and is largely similar to the first\\nstage. The main difference is that the input text\\nis concatenated with the predicted rationales from\\nthe first stage. It is worth noting that the above\\nprocess describes a general multimodal reasoning\\nframework. However, for text-only reasoning tasks,\\nthere are no image features, so the image encod-\\ning and vision feature fusion processes mentioned\\nabove can be omitted. In the following section,\\nwe will provide a detailed exposition of the two\\nkey steps of our GoT reasoning framework: GoT\\nconstruction and GoT encoding and feature fusion.\\n2.1\\nGoT Construction\\nGoT employs thought graphs to simulate human de-\\nductive reasoning, thereby modeling humans’ abil-\\nity for leaps of thought. Our aim is to reflect the\\nmost fundamental deduction process by construct-\\ning a thought graph. If we have evidence that x →\\ny and y →z, then it follows that x →z. In Fig-\\nure 3, the deduction reasoning can be formulated\\nas follows: Earthquake\\ncomes from\\n−\\n→\\n{earth, quake},\\n{earth, quake} means\\n−\\n→{ground, shake}. It is easy to\\nreason that Earthquake−\\n→{ground, shake}.\\nWe\\npropose\\na\\nnovel\\nExtract-Clustering-\\nCoreference (ECC) process to construct thought\\ngraphs.\\nECC first extracts deductive triplets\\nT = {ti = (ti\\nx, ti\\ny, ti\\nz)} as the discrete raw graph,\\nwhere ti\\nx, ti\\ny, and ti\\nz are thought units of the i-th\\ntriplet, and there exists an edge ei\\nxy between ti\\nx and\\nti\\ny, and an edge ei\\nyz between ti\\ny and ti\\nz. Then, ECC\\nclusters the nodes that refer to the same mentions\\nto conduct coreference resolution. Specifically,\\nwe replace every graph node that belongs to a\\ncoreference cluster with the most representative\\nmention in the cluster. By adopting this technique,\\nour model is better equipped with denser thought\\ngraphs and the ability for deductive reasoning. The\\ndetailed algorithm is illustrated in Algorithm 1.\\nIn GoT construction, during the rationale gen-\\neration stage, the input text consists of concate-\\nnated question, context, and choices. In multimodal\\nGoT, image caption (Lu et al., 2022) is appended\\nto the input text for GoT to incorporate image in-\\nThe word earthquake \\ncomes from the words \\nearth and quake. The word \\nearth means ground, and \\nthe word quake means to \\nshake.\\nEarthquake\\ncomes \\nfrom\\nearth\\nquake\\nmeans\\nground\\nshake\\nGoT\\nRationales\\nFigure 3: Graph-of-Thought deduction example\\nformation. During the answer inference stage, the\\npredicted rationales from the rationale generation\\nstage are further concatenated with the input text\\nfor corresponding GoT construction.\\nIn our implementation of ECC process, inspired\\nby (Chen and Yang, 2021), we utilize open in-\\nformation extraction (OpenIE) systems 2 (Angeli\\net al., 2015) to extract subject-verb-object triplets\\nas thought unit nodes. We apply coreference reso-\\nlution to the extracted nodes using the Stanford\\nCoreNLP system (Manning et al., 2014).\\nThe\\nconstructed thought graph is denoted as G(N, E),\\nwhere N represents the nodes extracted by OpenIE\\nand E represents the adjacency matrix. Rows and\\ncolumns correspond to the nodes in the graph, and\\nif there is an edge between two nodes, the corre-\\nsponding matrix element is 1; otherwise, it is 0.\\nAlgorithm 1 ECC process\\nInput: Input text S\\nOutput: Thought graph G(N, E)\\nExtract deductive triplet set T from S\\nT = {t0, t1, ..., tn}, ti = (ti\\nx, ti\\ny, ti\\nz)\\nfor every triplet ti ∈T do\\nNr ←Nr ∪{ti\\nx, ti\\ny, ti\\nz}\\nEr ←Er ∪{ei\\nxy, ei\\nyz}\\nend for\\nextract coreference clusters C for Nr\\nfor every node ni ∈Nr do\\nif ni ∈∀cj ∈C then\\nn∗\\nj ←most representative mention in cj\\nN ←N ∪{n∗\\nj}\\nend if\\nend for\\nReconnect N based on Er to construct E\\nreturn N , E\\n2https://github.com/philipperemy/Stanford-OpenIE-\\nPython\\n2.2\\nGoT Encoding and Integration\\nGoT reasoning utilizes separate encoders to encode\\ninput data for each modality. The thought graph\\nis encoded using a graph attention network, while\\nthe input text is encoded using a Transformer en-\\ncoder. In multimodal GoT reasoning, the image is\\nencoded using an additional vision encoder.\\n2.2.1\\nBase Encoder\\nText Encoder\\nFor text representation, we use the\\nTransformer encoder (e.g. T5 (Raffel et al., 2020a))\\nto encode the input text. Given input sentence S =\\n{w0, ..., wl}, we extract the hidden states from the\\nlast layer of the Transformer encoder to obtain the\\ntext representation HT :\\nHT = {h0, h1, ..., hl} = Encodertext(S)\\n(1)\\nwhere hi is the hidden representation of token i and\\nl represents the length of the text input.\\nVision Encoder (Optional)\\nFor multimodal rea-\\nsoning with vision modality, following (Zhang\\net al., 2023), we extract patch-level features of\\nimage I using readily available vision extraction\\nmodel as vision encoder Encodervision and then\\nemploy a trainable projection matrix WI to project\\nthe extracted features into the vision representation\\nHI which have the same shape with HT .\\nHI = WIEncodervision(I)\\n(2)\\n2.2.2\\nGoT Encoder\\nNode Embedding\\nWe first use special tokens\\n<s> and </s> to highlight every thought graph\\nnode.\\nSpecifically, for node set with j nodes\\nN = {n0, ...nj} , we construct the node input as p\\nand then feed the p into the same text encoder and\\nutilize the output representation of the special token\\n<s> as the initial node representation. Formally,\\np = [<s>, n0, </s>, ..., <s>, nj, </s>]\\n(3)\\n[hs\\n0, hn\\n0, he\\n0, ..., hs\\nj, hn\\nj , he\\nj] = Encodertext(p)\\n(4)\\nwhere the hs\\ni and he\\ni ∈RD are the representation\\nof <s> and </s> for node ni respectively, D is\\nthe dimension of node embedding, and the hn\\ni =\\n{hn\\ni,1, ..., hn\\ni,m} is the representations of node ni\\nwith m tokens. we use the hs\\ni to represent the node\\nrepresentation of ni.\\nDropout\\nGoT input\\nG 𝑁, 𝐸\\nGraph \\nAttention Layer\\nGraph \\nAttention Layer\\nConcatenate\\nDropout\\nGraph \\nAttention Layer\\nFFNN\\nLayernorm\\nGoT representation\\nMulti-head \\nattention\\nResidual connection\\nℎ𝑔′\\nℎ𝑔′\\n𝐻𝐺\\n…\\nFigure 4: Architecture of GoT encoder\\nGAT Encoder\\nWe employ a graph attention net-\\nwork (GAT) (Velickovic et al., 2018; Chen and\\nYang, 2021) to encode the thought graph. For every\\nnode ni in graph G(N, E), the graph attention\\nlayer is designed as:\\naij = Attention(\\n\\x02\\nWhs\\ni||Whs\\nj\\n\\x03\\n)\\n(5)\\nqij = LeakyReLU (aij)\\n(6)\\nαij = Softmax(qij) =\\nexp (qij)\\nP\\nk∈Ki exp (qik)\\n(7)\\nhg′\\ni = GELU\\n\\uf8eb\\n\\uf8edX\\nj∈Ki\\nαijWhs\\nj\\n\\uf8f6\\n\\uf8f8\\n(8)\\nwhere || denotes concatenate operation, the W is\\na trainable weight and the set Ki contains the node\\nni’s neighbours in thought graph G. Our graph\\nattention layer first employed a shared attention\\nmechanism Attention(.) : RD′ × RD′ →R to\\ncompute the attention weights, where D′ is the\\nattention layer output dimension. The attention\\nweights aij measures the importance of node ni’s\\nfeatures to nj’s features. By only calculating the\\nattention weights between nodes who are neigh-\\nbours, our graph attention layer demonstrates the\\nability to perceive structural information of graphs.\\nIn our implementation, we adopt a single-layer\\nfeed-forward neural network (FFNN) as the atten-\\ntion mechanism which is both simple and straight-\\nforward.\\nFigure 4 shows the architecture of our GoT en-\\ncoder. Our GoT encoder employs a multi-head\\ngraph attention layer, following (Velickovic et al.,\\n2018), we concatenate the output of each graph\\nattention layer and further pass it to a output graph\\nattention layer with the same architecture:\\nhg′\\ni = ∥K\\nk=1GELU\\n\\uf8eb\\n\\uf8edX\\nj∈Ni\\nαk\\nijWkhs\\nj\\n\\uf8f6\\n\\uf8f8\\n(9)\\nhg′′\\ni\\n= GELU\\n\\uf8eb\\n\\uf8edX\\nj∈Ni\\nαijWhg′\\nj\\n\\uf8f6\\n\\uf8f8\\n(10)\\nwhere K is the number of attention heads, || is\\nthe concatenate operation, and n is the number of\\nnodes in thought graph. We then use a single-layer\\nfeed-forward neural network (FFNN) to obtain the\\nfinal thought graph embedding HG:\\nhg′′ = [hg′′\\n0 , ..., hg′′\\nn ];\\nHG = FFNN(hg′′)\\n(11)\\n2.3\\nFeature Fusion\\nAfter obtaining the encoded features, we use a\\nsingle head attention to align the text representa-\\ntion HT with image representation HI and thought\\ngraph representation HG, respectively. The image\\nattention output HI and thought graph attention\\noutput HG are calculated by:\\nHI = Softmax\\n\\x12HT HI⊤\\n√\\nd\\n\\x13\\nHI\\n(12)\\nHG = Softmax\\n\\x12HT HG⊤\\n√\\nd\\n\\x13\\nHG\\n(13)\\nwhere Q is HT and d is the dimension of HT .\\nWe take both KI and VI as HI and KG and VG as\\nHG. Please note that image representation is op-\\ntional and is only required for multimodal dataset.\\nNext, a gated fusion mechanism (Wu et al., 2021;\\nZhang et al., 2023; Li et al., 2022; Zhang et al.,\\n2020) is applied to combine the attention outputs\\nHI and HG with the text representation HT . The\\nfeature fusion output H can be calculated by:\\nλ =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nSigmoid\\n\\x00WT HT + WGHG\\x01\\ntext-only\\nSigmoid\\n\\x00WT HT + WIHI + WGHG\\x01\\nmultimodal\\nH =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n(1 −λ) · HT + λ · HG\\ntext-only\\n(1 −λ) · HT + λ · HI + λ · HG\\nmultimodal\\nwhere WT ,WI and WG are all trainable weights.\\nWe then input the fused feature output H into the\\ndecoder to predict the rationales or the final answer.\\n3\\nExperiments\\nDataset\\nWe evaluate our model on the text-only\\nAQUA-RAT (Ling et al., 2017) and multimodal\\nScienceQA benchmark (Lu et al., 2022). The de-\\ntailed dataset information and statistics are shown\\nin Appendix B.\\nModel Setup\\nIn our experiments, we used\\nT5 (Raffel et al., 2020a) as our basic model ar-\\nchitecture, including both T5-base and T5-large\\nmodel sizes. Specifically, to ensure a fair compar-\\nison, we initialized our model with the finetuned\\nT5 checkpoint FLAN-Alpaca 3 and used ViT-large\\nencoder (Dosovitskiy et al., 2021) for the vision\\nencoder, following (Zhang et al., 2023). We fine-\\ntuned the models for 100 epochs with a learning\\nrate of 5e-5. The detailed training parameters are\\navailable in Appendix C. We trained our models on\\nfour NVIDIA A800 80G GPUs.\\n4\\nResults and Discussion\\n4.1\\nMain Results\\nBaselines\\nFor AQUA-RAT, our baselines include:\\n(1) Zero-Shot and Zero-Shot-CoT LLMs (Kojima\\net al., 2022); (2) Few-Shot and Manual-CoT LLMs\\n(Wei et al., 2022b) and Auto-CoT (Zhang et al.,\\n2022) (The above baselines all use the text-davinci-\\n002 version of GPT-3 with 175B parameters); (3)\\n3https://huggingface.co/declare-lab/flan-alpaca-base\\nFintuned LLMs: Calcformer-T5-L (Kadlˇ\\ncík et al.,\\n2023) which finetunes calculator-using T5-Large\\nmodel on the Calc-X collection. To have a fair com-\\nparison we also fine-tuned FLAN-Alpacabase and\\nFLAN-Alpacalarge on AQUA-RAT with traditional\\ntwo-stage CoT.\\nFor ScienceQA, following (Zhang et al., 2023;\\nLu et al., 2022), our adopted baselines include: (1)\\nVision question answering (VQA) baseline mod-\\nels (Yu et al., 2019; Anderson et al., 2018; Kim\\net al., 2018; Gao et al., 2019; Kim et al., 2021;\\nLu et al., 2021; Li et al., 2019, 2020); (2) Text-\\nto-text LLMs (Raffel et al., 2020b; Chen et al.,\\n2020) and (3) Text-to-text LLMs with CoT prompt-\\ning (Lu et al., 2022; Zhang et al., 2023). Both\\nUnifiedQA (Lu et al., 2022) and GPT-3.5 (Lu\\net al., 2022) use generated image captions to in-\\ncorporate vision semantics. Whereas, Mutimodal-\\nCoT (Zhang et al., 2023) injects generated image\\nfeatures into traditional CoT reasoning.\\nMODELS\\nTRAINING\\nSIZE\\nACC(%)\\nZero-Shot (Kojima et al., 2022)\\nzero-shot\\n175B\\n22.40\\nZero-Shot-CoT (Kojima et al., 2022)\\nzero-shot\\n175B\\n33.50\\nFew-Shot (Wei et al., 2022b)\\nfew-shot\\n175B\\n24.80\\nManual-CoT (Wei et al., 2022b)\\nfew-shot\\n175B\\n35.80\\nAuto-CoT (Zhang et al., 2022)\\nfew-shot\\n175B\\n36.50\\nCalcformer-T5-L (Kadlˇ\\ncík et al., 2023)\\ntrain-set\\n770M\\n27.20\\nFLAN-Alpacabase\\ntrain-set\\n223M\\n30.09 ± 1.12\\nGoT-T5base\\ntrain-set\\n223M\\n32.09 ± 1.62\\nFLAN-Alpacalarge\\ntrain-set\\n738M\\n33.73 ± 1.14\\nGoT-T5large\\ntrain-set\\n738M\\n34.48 ± 1.11\\nTable 1: Main test accuracy results (ACC%) of AQUA-\\nRAT. Size=backbone model size.\\nResults\\nThe rationales generation results can be\\nseen in Table 8 in Appendix D. The overall results\\nare reported in Table 1 and Table 2.\\nIn the AQUA-RAT dataset, our GoTbase model\\nattains a 0.78 enhancement in ROUGE-L scores for\\nrationale generation during the initial stage, outper-\\nforming the FLAN-Alpacabase model, which does\\nnot integrate GoT. For the answer generation phase,\\nthe GoTbase exhibits a substantial accuracy increase\\nof 2.00%, while the GoTlarge model records a 0.75%\\nenhancement. Compared to the 175B parameter\\nzero-shot and few-shot LLMs, our GoT-large, em-\\nploying just a 738M backbone model, achieves\\nresults remarkably close to those of Manual-CoT\\n(Wei et al., 2022b).\\nFor ScienceQA dataset, in rationale generation\\nstage, we can see from Table 8 that our model\\nachieves a ROUGE-L of 94.39 and outperforms\\nthe Mutimodal-CoTbase by 1.15. For the final an-\\nMODEL\\nTRAINING\\nSIZE\\nNAT\\nSOC\\nLAN\\nTXT\\nIMG\\nNO\\nG1-6\\nG7-12\\nAVG\\nHuman\\n-\\n-\\n90.23\\n84.97\\n87.48\\n89.60\\n87.50\\n88.10\\n91.59\\n82.42\\n88.40\\nVision question answering baselines\\nMCAN (Yu et al., 2019)\\ntrain-set\\n95M\\n56.08\\n46.23\\n58.09\\n59.43\\n51.17\\n55.40\\n51.65\\n59.72\\n54.54\\nTop-Down (Anderson et al., 2018)\\ntrain-set\\n70M\\n59.50\\n54.33\\n61.82\\n62.90\\n54.88\\n59.79\\n57.27\\n62.16\\n59.02\\nBAN (Kim et al., 2018)\\ntrain-set\\n112M\\n60.88\\n46.57\\n66.64\\n62.61\\n52.60\\n65.51\\n56.83\\n63.94\\n59.37\\nDFAF (Gao et al., 2019)\\ntrain-set\\n74M\\n64.03\\n48.82\\n63.55\\n65.88\\n54.49\\n64.11\\n57.12\\n67.17\\n60.72\\nViLT (Kim et al., 2021)\\ntrain-set\\n113M\\n60.48\\n63.89\\n60.27\\n63.20\\n61.38\\n57.00\\n60.72\\n61.90\\n61.14\\nPatch-TRM (Lu et al., 2021)\\ntrain-set\\n90M\\n65.19\\n46.79\\n65.55\\n66.96\\n55.28\\n64.95\\n58.04\\n67.50\\n61.42\\nVisualBERT (Li et al., 2019, 2020)\\ntrain-set\\n111M\\n59.33\\n69.18\\n61.18\\n62.71\\n62.17\\n58.54\\n62.96\\n59.92\\n61.87\\nText-to-text LLMs\\nUnifiedQAbase (Raffel et al., 2020b)\\nzero-shot\\n223M\\n68.16\\n69.18\\n74.91\\n63.78\\n61.38\\n77.84\\n72.98\\n65.00\\n70.12\\nGPT-3.5 (Chen et al., 2020)\\nzero-shot\\n175B\\n74.64\\n69.74\\n76.00\\n74.44\\n67.28\\n77.42\\n76.80\\n68.89\\n73.97\\nText-to-text LLMs with CoT\\nUnifiedQAbase (CoT) (Lu et al., 2022)\\nzero-shot\\n223M\\n71.00\\n76.04\\n78.91\\n66.42\\n66.53\\n81.81\\n77.06\\n68.82\\n74.11\\nGPT-3.5 (CoT) (Lu et al., 2022)\\n2-shot\\n175B\\n75.44\\n70.87\\n78.09\\n74.68\\n67.43\\n79.93\\n78.23\\n69.68\\n75.17\\nChatGPT (CoT) (Lu et al., 2023)\\nfew-shot\\n-\\n78.82\\n70.98\\n83.18\\n77.37\\n67.92\\n86.13\\n80.72\\n74.03\\n78.31\\nGPT-4 (CoT) (Lu et al., 2023)\\nfew-shot\\n-\\n85.48\\n72.44\\n90.27\\n82.65\\n71.49\\n92.89\\n86.66\\n79.04\\n83.99\\nMutimodal-CoTbase (Zhang et al., 2023)\\ntrain-set\\n223M\\n84.37\\n88.30\\n84.36\\n83.72\\n80.32\\n86.90\\n85.83\\n84.05\\n85.19\\nGoT-T5base\\ntrain-set\\n223M\\n86.25\\n93.55\\n85.51\\n85.89\\n86.30\\n86.34\\n87.79\\n87.23\\n87.59\\n± 0.31\\n± 0.06\\n± 0.11\\n± 0.32\\n± 0.28\\n± 0.12\\n± 0.10\\n± 0.40\\n± 0.20\\nMutimodal-CoTlarge (Zhang et al., 2023)\\ntrain-set\\n738M\\n91.03\\n93.70\\n86.64\\n90.13\\n88.25\\n89.48\\n91.12\\n89.26\\n90.45\\nGoT-T5large\\ntrain-set\\n738M\\n90.88\\n93.57\\n88.45\\n90.26\\n88.16\\n90.29\\n91.19\\n90.14\\n90.81\\n± 0.22\\n± 0.38\\n± 0.44\\n± 0.35\\n± 0.25\\n± 0.47\\n± 0.16\\n± 0.23\\n± 0.12\\nTable 2: Main test accuracy results (%) of ScienceQA. SIZE=backbone model size. Question classes: NAT =\\nnatural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO =\\nno context, G1-6 = grades 1-6, G7-12 = grades 7-12, AVG= average accuracy scores\\nswer generation stage, our GoT achieves SOTA in\\nall subjects and all grades. The most direct com-\\nparison is that our model achieves an accuracy of\\n87.59% which is 2.40% higher than that of the\\nMutimodal-CoTbase with the similar number of pa-\\nrameters.\\nGoT demonstrates a significant advantage over\\ntraditional CoT, elevating the accuracy from\\n30.09% to 32.09% in AQUA-RAT and from\\n85.19% to 87.59% in ScienceQA task. The results\\nsufficiently suggest that utilizing thought graph fea-\\ntures for deductive reasoning is a more effective\\napproach than the existing methods, which only\\nconsider text or vision features by simply incorpo-\\nrating image captions or fusing generated image\\nfeatures. In conclusion, our results confirm the\\neffectiveness of utilizing two-dimensional graph-\\nof-thought and demonstrate the potential of incor-\\nporating GoT into reasoning for LMs.\\n4.2\\nFurther Exploration\\n4.2.1\\nAblation Study\\nAQUA-RAT\\nIn order to make sure that intro-\\nducing thought graphs into GoT reasoning indeed\\nboost the performance, we conduct the following\\nexperiments:\\n(1) Random Thought Graph\\nIn the Random Thought Graph experiment, we\\nmaintain the GoT framework while introducing ran-\\ndomness into the process. We construct a thought\\ngraph by randomly selecting nodes and arbitrarily\\nestablishing connections between them. This ap-\\nproach is designed to evaluate the extent to which\\nthe GoT reasoning mechanism is reliant on the\\nstructured organization of thought graphs.\\n(2)\\nTriplets Concatenation In the Triplets Concate-\\nnation experiment, we take a straightforward ap-\\nproach by appending the extracted triplets directly\\nto the input text. This method aims to assess the\\nimpact of omitting the structural information typi-\\ncally provided by thought graphs, offering insight\\ninto the significance of this structural element in\\nthe reasoning process. (3) Coreference Injection\\nIn the Coreference Injection experiment, we ex-\\nplore the potential benefits of integrating corefer-\\nence resolution directly into the language model’s\\nreasoning process. We achieve this by incorpo-\\nrating coreference information into the input text,\\nwhere all instances of coreferent entities are re-\\nplaced with a consistent phrase, followed by model\\nfine-tuning. This experiment seeks to understand\\nthe role of coreference resolution in enhancing the\\nmodel’s deductive capabilities.\\nTable 3 shows the overall ablation results. From\\nthe table, we can see that by randomly construct\\nthought graphs to disrupt the deductive reasoning\\nprocess, our model suffers a loss of 1.78%, indicat-\\ning the effectiveness of GoT. The results of Triplets\\nMODEL\\nMODEL SIZE\\nACC\\n∆\\nGoT-T5base\\n233M\\n32.09\\n-\\nw/ Random Thought Graph\\n30.31\\n-1.78\\nTriplets Concatenation\\n233M\\n31.20\\n-0.89\\nCoreference Injection\\n233M\\n30.32\\n-1.77\\nTable 3:\\nAblation results of GoT on AQUA-RAT\\ndataset.\\nConcatenation on the AQUA-RAT showed an ac-\\ncuracy of 31.20%. This performance gap of 0.89\\nclearly demonstrates the significance of the struc-\\ntural information in our approach. For Coreference\\nInjection, the model suffers a loss of 1.77 % ac-\\ncuracy. We believe that these outcomes can be\\nattributed to a couple of factors: (1) Simply re-\\nplacing coreferent entities may lead to a loss of\\ncoherence in sentences, resulting in a reduction\\nof semantic information and consequently having\\na limited impact on overall accuracy. (2) Open\\nInformation Extraction (OpenIE) for coreference\\nresolution is not flawless, and direct replacement\\nof entities might introduce noise that misleads the\\nlanguage model during judgment.\\nContrastingly, the construction of a thought\\ngraph in the GoT framework does not compromise\\nthe original textual information (questions and ra-\\ntionales). Instead, it introduces additional structural\\nassistance for LMs to conduct reasoning effectively.\\nThus, we contend that GoT’s approach is indispens-\\nable and beneficial, as it supplements the LM’s\\ncomprehension without introducing potential noise\\nor loss of coherence in the input text.\\nScienceQA\\nTo examine the impact of different\\nbackbone and vision encoder configurations on the\\nGoT, we employed a distinct set of model settings.\\nMore specifically, we adopted the pre-trained T5\\ncheckpoint UnifiedQA (Khashabi et al., 2020) as\\nthe backbone model and utilized DETR (Carion\\net al., 2020) for the vision encoder, with results\\nillustrated in the Table 4. As shown, our GoT out-\\nperforms Mutimodal-CoT across various model\\nconfigurations. A comparison reveals that GoT\\ncan achieve greater improvements on smaller mod-\\nels. We believe the main reason is that when the\\nlanguage model is not as robust, or when employ-\\ning a relatively weaker vision encoder like DETR\\ncompared to ViT, GoT can leverage the inherent\\ninformation within the language to enhance per-\\nformance significantly. Additionally, to prove that\\nour GoT’s performance gain is not simply due to\\nan increase in parameters, we conducted an abla-\\ntion study. We expanded the parameter count of\\nMultimodal-CoTbase to match our 233M model\\nsize by adding two layers of MLP instead of one in\\nthe gated fusion module, referred to as Multimodal-\\nCoTbase(enlarged). We also constructed a random\\nthought graph ablation study on the ScienceQA\\ndataset. The results from the ablation studies can\\nbe observed in the table 4. From the table, it is\\nevident that our model significantly outperforms\\nthe enlarged Multimodal-CoT by an accuracy of\\n2.04%. These findings convincingly demonstrate\\nthe significance of incorporating thought graphs\\ninto multimodal reasoning. The performance of\\nGoT with a randomly constructed thought graph\\nwas even lower than Mutimodal-CoT, indicating\\nthat when the language model and vision encoder\\nare weaker, the model relies more heavily on GoT\\nfor reasoning.\\nModel\\nACC\\n∆\\nUnifiedQA+DETR\\nMutimodal-CoTbase\\n77.67\\n-\\nMutimodal-CoTlarge\\n81.37\\n-\\nGoTbase\\n81.21\\n3.54\\nGoTlarge\\n82.74\\n1.37\\nAblation Studies\\nMutimodal-CoTbase(enlarged)\\n79.17\\n-2.04\\nGoTbase w/ Random Thought Graph\\n76.74\\n-4.47\\nTable 4: Ablation results of GoT on ScienceQA dataset.\\nFor GoT models ∆indicates the performance gains of\\nGoT models over their Multimodal-CoT counterparts.\\nIn the ablation studies, ∆represents improvements rela-\\ntive to the GoTbase model\\n4.2.2\\nAnalysis\\nPerformance on Different Classes\\nIn order to\\ninvestigate the impact of GoT on the overall model\\nperformance across different subjects , we calcu-\\nlated the accuracy for different subjects and com-\\npared it with that of Mutimodal-CoT. We also\\ncompare the performance of two models on dif-\\nferent question classes.The radar Figure 5 shows\\nthe overall results for our base model. With re-\\nspect to various subjects and question classes, our\\nmodel demonstrates superior performance over the\\nMutimodal-CoTbase and attains a more consistent\\nand enhanced outcome. Our model presents out-\\nstanding advantages especially in the field of social\\nscience, with an accuracy improvement of 5.25%.\\nFor different question classes, our model demon-\\nstrates the largest improvement on questions involv-\\ning images. Our hypothesis is that by constructing\\na thought graph and integrating the three features of\\ntext, image, and thought graph, we can better align\\nthe textual and visual information for the model,\\nthus maximizing the utilization of visual informa-\\ntion and obtaining more accurate answers.\\nFigure 5: Performance on different question classes\\n2\\n4\\n6\\n8\\n10\\n12\\n60\\n70\\n80\\n90\\n100\\nGrades\\nAccuracy(%)\\nOursbase\\nMutimodal-CoTbase\\nFigure 6: Performance on different grades\\nPerformance on Different Grades\\nIt can be\\nseen from the Table 2 that Mutimodal-CoT expe-\\nrience a decrease in accuracy of 1.78 as the grade\\nlevel of the given question increases while GoT\\nonly has minor decrease of 0.56. We believe the\\nmain reason is that by incorporating GoT, models\\nacquires the ability for deductive reasoning and can\\nbetter comprehend the relationships between differ-\\nent entities and thus better understand the meaning\\nof the problems. Through this method, for higher-\\ngrade problems with greater complexity, the model\\ncan construct a thought graph to help itself gener-\\nate a more complete logical chain for deduction,\\nthereby generating more accurate answers. More\\ndetailed model performance on different grades can\\nbe found in Figure 6. We can see that in the lower\\ngrade, two models achieves a similar performance.\\nAs the grade level increases and the difficulty of\\nthe questions becomes more challenging, the gap\\nbetween our model and the Mutimodal-CoT model\\ngradually widens. Due to the small number of ques-\\ntions (≤130) available for each grade in grade 1\\nand grades 11-12, there is greater fluctuation in the\\naccuracy of both models. Nevertheless, it is evident\\nfrom the table that our model exhibits stronger and\\nmore stable advantages over Mutimodal-CoT in\\neach grade.\\nCase Study and Limitation\\nIn order to gain a\\ndeeper understanding of the performance of GoT,\\nwe conduct case studies which can be found in\\nthe Appendix E. We also visualize the attention\\nweights aij in GoT encoder to demonstrate how\\nGoT performs deductive reasoning to generate\\nmore accurate answers in Appendix F. For the lim-\\nitation of this work, compared to CoT, GoT may\\nresult in additional computational costs and slightly\\nslower training times. Detailed limitation analysis\\ncan be found in Appendix G.\\n5\\nConclusion\\nWe introduce a novel Graph-of-Thought (GoT) rea-\\nsoning approach, which is an innovative method\\nfor modeling the non-sequential nature of human\\nthinking for LMs. GoT enhances LMs with deduc-\\ntive reasoning abilities, providing a more realistic\\nrepresentation of thought processes. Our exper-\\niments showcases the superiority of GoT on the\\ntext-only reasoning dataset AQUA-RAT, achieving\\na similar result compared to GPT-3 model while\\nutilizing significantly fewer parameters. Further-\\nmore, GoT establishes a new state-of-the-art on\\nthe multimodal reasoning benchmark, ScienceQA\\nwith fewer parameters. This performance surpasses\\nstrong ChatGPT and GPT-4 systems, as well as hu-\\nman performance, demonstrating the efficacy of\\nGoT. Through comprehensive case studies and ab-\\nlation studies, we provide substantial evidence of\\nthe effectiveness of GoT in reasoning tasks. If you\\nwant it, you GoT it!\\nReferences\\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\\n2018. Bottom-up and top-down attention for image\\ncaptioning and visual question answering. In 2018\\nIEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2018, Salt Lake City, UT, USA,\\nJune 18-22, 2018, pages 6077–6086. Computer Vi-\\nsion Foundation / IEEE Computer Society.\\nGabor Angeli, Melvin Jose Johnson Premkumar, and\\nChristopher D. Manning. 2015. Leveraging linguis-\\ntic structure for open domain information extraction.\\nIn Proceedings of the 53rd Annual Meeting of the As-\\nsociation for Computational Linguistics and the 7th\\nInternational Joint Conference on Natural Language\\nProcessing (Volume 1: Long Papers), pages 344–354,\\nBeijing, China. Association for Computational Lin-\\nguistics.\\nLawrence W Barsalou. 1999. Perceptual symbol sys-\\ntems. Behavioral and brain sciences, 22(4):577–660.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\\nNicolas Usunier, Alexander Kirillov, and Sergey\\nZagoruyko. 2020. End-to-end object detection with\\ntransformers. In Computer Vision - ECCV 2020 -\\n16th European Conference, Glasgow, UK, August\\n23-28, 2020, Proceedings, Part I, volume 12346 of\\nLecture Notes in Computer Science, pages 213–229.\\nSpringer.\\nJiaao Chen and Diyi Yang. 2021. Structure-aware ab-\\nstractive conversation summarization via discourse\\nand action graphs. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, NAACL-HLT 2021, Online, June\\n6-11, 2021, pages 1380–1391. Association for Com-\\nputational Linguistics.\\nTing Chen, Simon Kornblith, Kevin Swersky, Moham-\\nmad Norouzi, and Geoffrey E. Hinton. 2020. Big\\nself-supervised models are strong semi-supervised\\nlearners. In Advances in Neural Information Pro-\\ncessing Systems 33: Annual Conference on Neural\\nInformation Processing Systems 2020, NeurIPS 2020,\\nDecember 6-12, 2020, virtual.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2022. Palm: Scaling language mod-\\neling with pathways. CoRR, abs/2204.02311.\\nAlexey\\nDosovitskiy,\\nLucas\\nBeyer,\\nAlexander\\nKolesnikov,\\nDirk Weissenborn,\\nXiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\\nUszkoreit, and Neil Houlsby. 2021.\\nAn image\\nis worth 16x16 words:\\nTransformers for image\\nrecognition at scale. In 9th International Conference\\non Learning Representations, ICLR 2021, Virtual\\nEvent, Austria, May 3-7, 2021. OpenReview.net.\\nPeng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu,\\nSteven C. H. Hoi, Xiaogang Wang, and Hongsheng\\nLi. 2019.\\nDynamic fusion with intra- and inter-\\nmodality attention flow for visual question answer-\\ning. In IEEE Conference on Computer Vision and\\nPattern Recognition, CVPR 2019, Long Beach, CA,\\nUSA, June 16-20, 2019, pages 6639–6648. Computer\\nVision Foundation / IEEE.\\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\\nCui, Owais Khan Mohammed, Barun Patra, Qiang\\nLiu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\\nVishrav Chaudhary, Subhojit Som, Xia Song, and\\nFuru Wei. 2023.\\nLanguage is not all you need:\\nAligning perception with language models. CoRR,\\nabs/2302.14045.\\nMarek Kadlˇ\\ncík, Michal Štefánik, Ondrej Sotolar, and\\nVlastimil Martinek. 2023. Calc-X and calcformers:\\nEmpowering arithmetical chain-of-thought through\\ninteraction with symbolic systems. In Proceedings\\nof the 2023 Conference on Empirical Methods in\\nNatural Language Processing, pages 12101–12108,\\nSingapore. Association for Computational Linguis-\\ntics.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\\naries with a single QA system. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2020, Online Event, 16-20 November 2020, volume\\nEMNLP 2020 of Findings of ACL, pages 1896–1907.\\nAssociation for Computational Linguistics.\\nJin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.\\n2018. Bilinear attention networks. In Advances in\\nNeural Information Processing Systems 31: Annual\\nConference on Neural Information Processing Sys-\\ntems 2018, NeurIPS 2018, December 3-8, 2018, Mon-\\ntréal, Canada, pages 1571–1581.\\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\\nVision-and-language transformer without convolu-\\ntion or region supervision. In Proceedings of the\\n38th International Conference on Machine Learning,\\nICML 2021, 18-24 July 2021, Virtual Event, volume\\n139 of Proceedings of Machine Learning Research,\\npages 5583–5594. PMLR.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022.\\nLarge\\nlanguage models are zero-shot reasoners.\\nCoRR,\\nabs/2205.11916.\\nBei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong\\nXiao, Anxiang Ma, and JingBo Zhu. 2022. On vi-\\nsion features in multimodal machine translation. In\\nProceedings of the 60th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 6327–6337, Dublin, Ireland. As-\\nsociation for Computational Linguistics.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\\nple and performant baseline for vision and language.\\nCoRR, abs/1908.03557.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\\nHsieh, and Kai-Wei Chang. 2020. What does BERT\\nwith vision look at?\\nIn Proceedings of the 58th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\\npages 5265–5275. Association for Computational\\nLinguistics.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 158–167, Vancouver,\\nCanada. Association for Computational Linguistics.\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:\\nMultimodal reasoning via thought chains for science\\nquestion answering. In The 36th Conference on Neu-\\nral Information Processing Systems (NeurIPS).\\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\\nsitional reasoning with large language models. CoRR,\\nabs/2304.09842.\\nPan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou\\nZhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and\\nSong-Chun Zhu. 2021. Iconqa: A new benchmark\\nfor abstract diagram understanding and visual lan-\\nguage reasoning. In Proceedings of the Neural In-\\nformation Processing Systems Track on Datasets and\\nBenchmarks 1, NeurIPS Datasets and Benchmarks\\n2021, December 2021, virtual.\\nChristopher Manning, Mihai Surdeanu, John Bauer,\\nJenny Finkel, Steven Bethard, and David McClosky.\\n2014. The Stanford CoreNLP natural language pro-\\ncessing toolkit. In Proceedings of 52nd Annual Meet-\\ning of the Association for Computational Linguis-\\ntics: System Demonstrations, pages 55–60, Balti-\\nmore, Maryland. Association for Computational Lin-\\nguistics.\\nOpenAI. 2023. Gpt-4 technical report.\\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,\\nShaohan Huang, Shuming Ma, and Furu Wei. 2023.\\nKosmos-2: Grounding multimodal large language\\nmodels to the world. CoRR, abs/2306.14824.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020a. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020b. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\\n2018. Graph attention networks. In 6th International\\nConference on Learning Representations, ICLR 2018,\\nVancouver, BC, Canada, April 30 - May 3, 2018,\\nConference Track Proceedings. OpenReview.net.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\\nLe, Ed H. Chi, and Denny Zhou. 2022.\\nSelf-\\nconsistency improves chain of thought reasoning in\\nlanguage models. CoRR, abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022a.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. CoRR, abs/2201.11903.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\\nand Denny Zhou. 2022b. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems\\n35: Annual Conference on Neural Information Pro-\\ncessing Systems 2022, NeurIPS 2022, New Orleans,\\nLA, USA, November 28 - December 9, 2022.\\nZhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, and\\nBen Kao. 2021. Good for misconceived reasons: An\\nempirical revisiting on the need for visual context\\nin multimodal machine translation. In Proceedings\\nof the 59th Annual Meeting of the Association for\\nComputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing\\n(Volume 1: Long Papers), pages 6153–6166, Online.\\nAssociation for Computational Linguistics.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L Griffiths,\\nYuan Cao,\\nand Karthik\\nNarasimhan. 2023.\\nTree of thoughts: Deliberate\\nproblem solving with large language models. arXiv\\npreprint arXiv:2305.10601.\\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.\\n2019. Deep modular co-attention networks for visual\\nquestion answering. In IEEE Conference on Com-\\nputer Vision and Pattern Recognition, CVPR 2019,\\nLong Beach, CA, USA, June 16-20, 2019, pages 6281–\\n6290. Computer Vision Foundation / IEEE.\\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\\n2020.\\nNeural machine translation with universal\\nvisual representation. In 8th International Confer-\\nence on Learning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\\nZhuosheng Zhang and Aston Zhang. 2023. You only\\nlook at screens: Multimodal chain-of-action agents.\\nCoRR, abs/2309.11436.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2022. Automatic chain of thought prompting\\nin large language models. CoRR, abs/2210.03493.\\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\\nGeorge Karypis, and Alex Smola. 2023.\\nMulti-\\nmodal chain-of-thought reasoning in language mod-\\nels. CoRR, abs/2302.00923.\\nAppendix\\nA\\nRelated Works\\nIn chain-of-thought reasoning, one idea leads to the\\nnext in a logical sequence and builds on previous\\nknowledge. Each idea is supported by evidence\\nor reasoning, and the conclusions drawn from the\\nchain are logical and sound. Most CoT methods\\ncan be divided into two categories based on how to\\ngenerate the final answer: (1) prompting for CoT,\\nincluding zero-shot CoT and few-shot CoT; and (2)\\nfine-tuning for CoT.\\nZero-shot CoT Prompting\\nAs large language\\nmodels continue to advance rapidly, many re-\\nsearchers are beginning to explore CoT reasoning\\nfor LLMs. The zero-shot CoT method proposed\\nby Kojima et al. (2022) consists of two stages: (1)\\nadding a \"Let’s think step by step\" prompt to gener-\\nate CoT, and (2) concatenating the generated CoT\\nand adding the phrase \"So the answer is\" to ob-\\ntain the final answer. Tree-of-Thought (ToT) (Yao\\net al., 2023) enables deliberate decision-making\\nthrough exploration of coherent text units. ToT di-\\nvides thoughts into thought units and models them\\nas a tree-like search process. Although both GoT\\nand ToT aim to capture human non-linear thoughts,\\nGoT is distinct from ToT in terms of both methodol-\\nogy and objectives. We believe that human thinking\\ninvolves both linear and non-linear aspects. Thus,\\nwe build upon the linear CoT framework by in-\\ncorporating non-linear structures to simultaneously\\ncapture both linear and non-linear human reason-\\ning. Tree-of-thoughts focuses on modeling non-\\nlinear thoughts explicitly, whereas our approach\\nleverages non-linear structures to assist the Chain-\\nof-Thought reasoning.\\nFew-shot CoT Prompting\\nFew-shot CoT rea-\\nsoning for LLMs, however, utilizes multiple input-\\noutput pairs to prompt the LLMs to output CoT\\nand obtain the final answer. Due to its ability to\\nprovide better performance compared to Zero-shot\\nCoT, Few-shot CoT has gained more attention in\\nresearch, particularly through effective demonstra-\\ntions. Few-shot CoT prompting was first formally\\nexplored by Wei et al. (2022a) and is a form of dis-\\ncrete prompt learning that involves context learning\\nin large models. Compared to traditional in-context\\nlearning, which prompts LLMs with a list of input-\\noutput demonstration pairs along with a test input\\nto allow the model to predict output, Few-shot CoT\\nprompting outputs additional logical reasoning pro-\\ncedures apart from the target output. Wang et al.\\n(2022) proposed a follow-up method to (Wei et al.,\\n2022a). The main improvement is that the model\\nuses the majority vote for the answers, which was\\nfound to significantly improve the performance of\\nthe CoT. However, these few-shot CoT models de-\\npend on hand-crafted demonstrations. To solve this\\nproblem, Zhang et al. (2022) proposed Auto-CoT,\\nwhich maintains the diversity of sampled questions\\nand generates reasoning chains to automatically\\nconstruct demonstrations. Specifically, Auto-CoT\\nconsists of two main stages: (1) Problem clustering:\\ndivide the given dataset of problems into several\\nclusters; (2) Demonstration sampling: select a rep-\\nresentative problem from each cluster and use a\\nsimple heuristic method to generate its reasoning\\nchain. Furthermore, Lu et al. (2023) also explores\\nfew-shot CoT reasoning for recently popular LLMs\\nChatGPT and GPT-4.\\nCoT Fine-tuning\\nIn Zhang et al. (2023), it was\\nproposed to fine-tune smaller language models in-\\nstead of prompting them in LLMs. And this ap-\\nproach enabled the CoT to go beyond textual infor-\\nmation and incorporate visual (image) modalities\\nusing a gated fusion mechanism into a two-stage\\nCoT. The results demonstrated that CoT fine-tuning\\nwith fewer parameters has potential. Therefore, in\\nthis work, we focus on fine-tuning for CoT to re-\\nduce the number of required model parameters and\\nhelp LLMs better comprehend different modalities.\\nHowever, previous CoT research has been limited\\nto different modalities, such as textual and vision\\ninformation, without considering the deduction rea-\\nsoning process. Therefore, in this work, we move\\nbeyond modeling the reasoning process solely as\\na thought chain and elevate it to a thought graph.\\nWe provide a more comprehensive and nuanced\\nrepresentation, enabling LLMs to perceive the de-\\nduction reasoning process accurately, resulting in\\nmore precise answer generation.\\nB\\nDataset\\nAQUA-RAT dataset consists of about 100,000 al-\\ngebraic word problems with natural language ra-\\ntionales. For AQUA-RAT, the model is trained to\\nreasoning through the steps to generate the final\\nanswer. ScienceQA benchmark is the pioneering\\nlarge-scale dataset for multimodal science ques-\\ntions, equipped with comprehensive annotations for\\nanswers, including detailed lectures and explana-\\ntions. The dataset contains 21k questions covering\\nthree subjects: natural science, language science,\\nand social science. Each question is presented with\\na context in the form of natural language or an\\noptional image. The model is trained to elucidate\\nthe reasoning process in natural language while\\nchoosing the answer from a set of options.\\nSplits\\n#Problems\\nTrain\\n97467\\nDev\\n254\\nTest\\n254\\nTable 5: AQUA-RAT dataset statistics (# denotes num-\\nbers)\\nStatistic\\nNumber\\nSplits\\n#Train\\n12,726\\n#Dev\\n4,241\\n#Test\\n4,241\\n#Total\\n21,208\\nAttribute\\n#Subjects\\n3\\n#Topic\\n26\\n#Category\\n127\\n#Skill\\n379\\nTable 6: ScienceQA dataset statistics (# denotes num-\\nbers)\\nC\\nTraining Parameters\\nParameters\\nValue\\nEpochs\\n100\\nBatch size for T5-base (per device)\\n10\\nBatch size for T5-large (per device)\\n8\\nLearning rate\\n5e-5\\nWeight decay\\n0.01\\nMax input length\\n512\\nMax number of nodes\\n150\\nTable 7: Training parameters for GoT\\nD\\nRationale Generation Results\\nThe rationale genration results can be found in Ta-\\nble 8. We can observe from Table 8 that the im-\\npact of GoT on rationale generation is limited. We\\nattribute this limitation to the fact that the input\\ntext for thought graph construction only includes\\nquestions and choices. Consequently, the thought\\ngraph constructed from such limited information\\ncan only facilitate constrained deductive reasoning.\\nHowever, in the answer generation stage, when pro-\\nvided with rationales, the model needs to possess\\nstronger deductive reasoning capabilities to under-\\nstand the relationship between rationales, questions,\\nand choices.\\nE\\nCase Study\\nTo facilitate a more illustrative comparison between\\nGoT and the CoT, we have selected several repre-\\nsentative examples. Figure 7 illustrates the exam-\\nples from AQUA-RAT dataset. Figure 8 to Figure\\n11 illustrates examples from ScienceQA dataset.\\nFrom Figure 8 and Figure 9, we can see that GoT\\ncan better understand the rationales and generate\\nmore accurate result. In Figure 10, we can see that\\nwhen provided with wrong rationale, our model is\\nmore robust to the noise and can focus on more\\nimportant key information. (We highlight the noisy\\nwrong rationale in red and correct key rationale\\nin green). Figure 11 presents a language prob-\\nlem which have less context and requires a certain\\namount of common sense knowledge. Hence, the\\nimpact of constructing a mind map on enhancing\\nthe model is not significant. Therefore, both GoT\\nand CoT predict wrong answers.\\nF\\nRepresentation Visualization\\nIn order to demonstrate the deductive reasoning\\nprocess of GoT more intuitively, we visualized the\\nattention weights of the GoT encoder. The visu-\\nalization results can be found in Figure 12. We\\ntook Figure 10 as an example. In Figure 10, even\\ngiven a wrong rationale, GoT still manages to gen-\\nerate the right answer. We select 14 representative\\nthought nodes and found that \"blue\",\"color\", and\\n\"common\" have the greatest weights which indi-\\ncates that GoT guides the model to focus on more\\nimportant words and conduct correct deductive rea-\\nsoning. For the disruptive node \"a hard object,\" our\\nmodel can effectively discriminate against it and as-\\nsign a lower attention weight to prevent the model\\nfrom selecting incorrect answers, as traditional CoT\\nmodels often do due to erroneous rationales.\\nG\\nLimitation\\nCompared to Mutimodal-CoT (Zhang et al., 2023),\\nincorporating GoT may result in additional com-\\nputational costs and slightly slower training times.\\nMODELS\\nBLEU1\\nBLEU4\\nROUGE\\nSIMILARITY\\nAQUA-RAT\\nFLAN-Alpacabase\\n19.78\\n3.49\\n28.40\\n68.61\\nFLAN-Alpacalarge\\n22.45\\n5.40\\n29.55\\n70.34\\nGoT-T5base\\n22.05\\n5.02\\n29.18\\n69.09\\nGoT-T5large\\n24.47\\n6.68\\n29.86\\n71.58\\nScienceQA\\nMutimodal-CoT∗\\nbase (Zhang et al., 2023)\\n91.04\\n86.81\\n93.24\\n96.34\\nGoT-T5base\\n92.50\\n88.79\\n94.39\\n96.74\\nGoT-T5large\\n93.49\\n90.09\\n95.17\\n97.33\\nTable 8: Rationale generation results (%). (*: we re-run the Mutimodal-CoTbase to report the full rationale scores.\\nWe use sentence-transformers (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to obtain sentence\\nembeddings and calculate the cosine similarity for SIMILARITY)\\nThe training parameters and inference times of the\\ndifferent models are presented in Table 9, which\\nreveals that our model requires a 0.2% increase in\\nparameters compared to Mutimodal-CoT.\\n#Parameters\\nInference time\\n(eval samples/per second)\\nMutimodal-CoTbase\\n227M\\n16.33\\nOurs\\n233M\\n13.38\\nTable 9: The number of training parameters and infer-\\nence time of different models (# denotes numbers)\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: paper is in a square form whose one side is 20 cm. Two semi circles \\nare drawn on its opposites as diameters. If these semi circles are cut down \\nwhat is the area of the remaining paper\\nChoices:\\n(A) 8.73\\n(B) 8.79\\n(C) 8.75\\nRationale: (5 * 3.5)/2 = 8.75\\nAnswer: C\\nRationale: Area of the square = pi*r2/4 = (20*20)/(2*2) = 8.73\\n  \\nAnswer: A\\nRationale: Explanation: Area of the paper = (r2 + r2) = (20 x 20) cm2 = 1200 cm2. Area of \\ntwo semi circles cut down = [(20 - r2)2 - (20 x 20)2] cm2 = (1200 - 225) cm2 = 875 cm2.\\nAnswer: C\\n(D) 8.71\\nwrong rationales right answer\\nwrong rationales wrong answer\\n(E) 8.72\\nFigure 7: Examples of AQUA-RAT\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: Would you find the word pink on a dictionary page with the following guide words?\\nparrot – property\\nChoices:\\n(A) yes\\n(B) no\\nRationale: lecture: Guide words appear on each page of a dictionary. They tell you the first\\nword and last word on the page. The other words on the page come between the guide\\nwords in alphabetical order. To put words in alphabetical order, put them in order by their\\nfirst letters. If the first letters are the same, look at the second letters. If the second letters\\nare the same, look at the third letters, and so on. If one word is shorter, and there are no\\nmore letters to compare, then the shorter word comes first in alphabetical order. For\\nexample, be comes before bed.\\nsolution: Put the words in alphabetical order. Since pink is between the guide words parrot -\\nproperty, it would be found on that page.\\nAnswer: The answer is (A)\\nRationale: Solution: Guide words appear on each page of a dictionary. They tell you the first\\nword and last word on the page. The other words on the page come between the guide\\nwords in alphabetical order. To put words in alphabetical order, put them in order by their\\nfirst letters. If the first letters are the same, look at the second letters. If the second letters\\nare the same, look at the third letters, and so on. If one word is shorter, and there are no\\nmore letters to compare, then the shorter word comes first in alphabetical order. For\\nexample, be comes before bed. Put the words in alphabetical order. Since pink is between\\nthe guide words parrot - property, it would be found on that page.\\nAnswer: The answer is (A)\\nRationale: Solution: Guide words appear on each page of a dictionary. They tell you the first\\nword and last word on the page. The other words on the page come between the guide\\nwords in alphabetical order. To put words in alphabetical order, put them in order by their\\nfirst letters. If the first letters are the same, look at the second letters. If the second letters\\nare the same, look at the third letters, and so on. If one word is shorter, and there are no\\nmore letters to compare, then the shorter word comes first in alphabetical order. For\\nexample, be comes before bed. Put the words in alphabetical order. Since pink is between\\nthe guide words parrot - property, it would be found on that page.\\nAnswer: The answer is (B)\\nright rationales right answer\\nright rationales wrong answer\\nFigure 8: Examples of ScienceQA\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: What is the name of the colony shown?\\nChoices:\\n(A) New Hampshire\\n(B) Vermont\\n(C) New York\\nRationale: The colony is New York. During the\\ncolonial era, New Hampshire and New York both\\nclaimed the territory that would later become the\\nstate of Vermont. Vermont was never its own\\ncolony.\\nAnswer: The answer is (C)\\nRationale: Solution: The colony is New York. During the colonial era, New Hampshire and\\nNew York both claimed the territory that would later become the state of Vermont.\\nVermont was never its own colony.\\nAnswer: The answer is (C)\\nRationale: Solution: The colony is Delaware York. During the colonial era, New Hampshire\\nand New York both claimed the territory that would later become the state of Vermont.\\nVermont was never its own colony.\\nAnswer: The answer is (D)\\n(D) Delaware\\nright rationales right answer\\nwrong rationales wrong answer\\nFigure 9: Examples of ScienceQA\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: Which property do these three \\nobjects have in common?\\nChoices:\\n(A) blue\\n(B) hard\\n(C) sticky\\nRationale: lecture: An object has different properties. A property of an object can tell you\\nhow it looks, feels, tastes, or smells. Properties can also tell you how an object will behave\\nwhen something happens to it. Different objects can have properties in common. You can\\nuse these properties to put objects into groups.\\nSolution: Look at each object. For each object, decide if it has that property. A sticky object\\ncan attach or stick to other things. The toothpaste is sticky, but the soccer shorts and the\\nwater slide are not. Blue is a color. This color is blue. All three objects are blue. A hard object\\ndoes not change shape when pressed or squeezed. The water slide is hard, but the\\ntoothpaste and the soccer shorts are not. The property that all three objects have in\\ncommon is blue.\\nAnswer: The answer is (A)\\nRationale: Solution: An object has different properties. A property of an object can tell you\\nhow it looks, feels, tastes, or smells. Properties can also tell you how an object will behave\\nwhen something happens to it. Different objects can have properties in common. You can\\nuse these properties to put objects into groups. Look at each object. For each object, decide\\nif it has that property. A hard object can attach or stick to other things. The is sticky, but the\\nshorts and the pitcher are not. Blue is a color. This color is blue. The three objects are blue.\\nA hard object does not change shape when pressed or squeezed. The tennis slide and hard,\\nbut the tennis and the water shorts are not. The property that all three objects have in\\ncommon is blue.\\nAnswer: The answer is (A)\\nRationale: Solution: An object has different properties. A property of an object can tell you\\nhow it looks, feels, tastes, or smells. Properties can also tell you how an object will behave\\nwhen something happens to it. Different objects can have properties in common. You can\\nuse these properties to put objects into groups. Look at each object. For each object, decide\\nif it has that property. A sticky object can attach or stick to other things. The is sticky, but the\\nshorts and the blue bottle are not. Blue is a color. This color is blue. None three objects are\\nblue. A hard object does not change shape when pressed or squeezed. None tennis slide\\nand hard, but the is the water shorts are not. The property that all three objects have in\\ncommon is sticky.\\nAnswer: The answer is (C)\\nwrong rationales right answer\\nwrong rationales wrong answer\\nFigure 10: Examples of ScienceQA\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: Select the action that doesn\\'t belong\\nChoices:\\n(A) chop\\n(B) blend\\n(C) stir\\nRationale: Chop doesn\\'t belong. Blend, mix, and stir all describe ways to combine\\nthings\\nAnswer: The answer is (A)\\nRationale: Solution: Mixp doesn‘t belong. Murend, chop, and chop all name things to get\\nthings.\\nAnswer: The answer is (D)\\nRationale: Solution: Blendp doesn\\'t belong..Murend, chop, and blend all name things to\\ngetAnswer: The answer is (B)\\n(D) mix\\nwrong rationales wrong answer\\nwrong rationales wrong answer\\nFigure 11: Examples of ScienceQA\\nthree objects\\nhave in\\ncommon\\nobject\\nhas\\ndifferent properties\\nput objects into\\ngroups\\na hard object\\ncan attach to\\nother things\\nis\\ncolor\\nblue\\n49.56\\n44.00\\nFigure 12: Representation visualization\\n')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f18bc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T16:37:52.996493400Z",
     "start_time": "2024-10-05T16:37:52.988354400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'Published': '2023-11-15',\n 'Title': 'Contrastive Chain-of-Thought Prompting',\n 'Authors': 'Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing',\n 'Summary': 'Despite the success of chain of thought in enhancing language model\\nreasoning, the underlying process remains less well understood. Although\\nlogically sound reasoning appears inherently crucial for chain of thought,\\nprior studies surprisingly reveal minimal impact when using invalid\\ndemonstrations instead. Furthermore, the conventional chain of thought does not\\ninform language models on what mistakes to avoid, which potentially leads to\\nmore errors. Hence, inspired by how humans can learn from both positive and\\nnegative examples, we propose contrastive chain of thought to enhance language\\nmodel reasoning. Compared to the conventional chain of thought, our approach\\nprovides both valid and invalid reasoning demonstrations, to guide the model to\\nreason step-by-step while reducing reasoning mistakes. To improve\\ngeneralization, we introduce an automatic method to construct contrastive\\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\\ncontrastive chain of thought can serve as a general enhancement of\\nchain-of-thought prompting.',\n 'entry_id': 'http://arxiv.org/abs/2311.09277v1',\n 'published_first_time': '2023-11-15',\n 'comment': None,\n 'journal_ref': None,\n 'doi': None,\n 'primary_category': 'cs.CL',\n 'categories': ['cs.CL'],\n 'links': ['http://arxiv.org/abs/2311.09277v1',\n  'http://arxiv.org/pdf/2311.09277v1']}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 메타데이터 출력\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58a907",
   "metadata": {},
   "source": [
    "`load_all_available_meta=False` 인 경우 메타데이터는 전체가 아닌 일부만 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb84ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 에 검색하고자 하는 논문의 주제를 입력합니다.\n",
    "loader = ArxivLoader(\n",
    "    query=\"ChatGPT\",\n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=False,  # 메타데이터 전체 로드 여부\n",
    ")\n",
    "\n",
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 메타데이터 출력\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70249aec",
   "metadata": {},
   "source": [
    "## 요약(summary)\n",
    "\n",
    "- 논문의 전체 내용이 아닌 요약본을 출력하고자 한다면, `get_summaries_as_docs()` 함수를 호출하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 요약 로딩\n",
    "docs = loader.get_summaries_as_docs()\n",
    "\n",
    "# 첫 번째 문서 접근\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788cb47",
   "metadata": {},
   "source": [
    "## lazy_load()\n",
    "\n",
    "문서를 대량으로 로드할 때 모든 로드된 문서의 부분 집합에 대해 하류 작업을 수행할 수 있다면, 메모리 사용량을 최소화하기 위해 문서를 한 번에 하나씩 지연 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "# 문서 지연 로드\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37793c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
